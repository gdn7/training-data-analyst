{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "1.16.3\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and modules\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "np.set_printoptions(threshold = np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data_genertator = False\n",
    "\n",
    "percent_sequence_before_anomaly = 70.0\n",
    "percent_sequence_after_anomaly = 0.0\n",
    "\n",
    "def create_time_series_normal_parameters():\n",
    "  normal_freq_noise_scale = 1.0\n",
    "  normal_frequence_noise_shift = 1.0\n",
    "\n",
    "  normal_ampl_noise_scale = 1.0\n",
    "  normal_ampl_noise_shift = 1.0\n",
    "\n",
    "  normal_noise_noise_scale = 1.0\n",
    "\n",
    "  if simple_data_genertator == True:\n",
    "    normal_freq = 1.0\n",
    "    normal_ampl = 1.0\n",
    "  else:\n",
    "    normal_freq = np.random.random() * normal_freq_noise_scale + normal_frequence_noise_shift\n",
    "    normal_ampl = np.random.random() * normal_ampl_noise_scale + normal_ampl_noise_shift\n",
    "  \n",
    "  return {\"normal_freq\": normal_freq, \n",
    "          \"normal_ampl\": normal_ampl, \n",
    "          \"normal_noise_noise_scale\": normal_noise_noise_scale}\n",
    "  \n",
    "def create_time_series_normal(\n",
    "  number_of_sequences, \n",
    "  seq_len, \n",
    "  normal_freq, \n",
    "  normal_ampl, \n",
    "  normal_noise_noise_scale):\n",
    "  # Normal parameters\n",
    "  if simple_data_genertator == True:\n",
    "    sequence = np.stack(\n",
    "      arrays = [np.sin(np.arange(0, seq_len) * normal_freq) * normal_ampl \n",
    "                    for _ in range(number_of_sequences)], axis = 0)\n",
    "  else:\n",
    "    sequence = np.stack(\n",
    "      arrays = [np.sin(np.arange(0, seq_len) * normal_freq) * normal_ampl + \\\n",
    "          [np.random.random() * normal_noise_noise_scale \n",
    "           for i in range(seq_len)] for _ in range(number_of_sequences)], axis = 0)\n",
    "\n",
    "  return sequence\n",
    "\n",
    "def create_time_series_with_anomaly(\n",
    "  number_of_sequences, \n",
    "  seq_len, \n",
    "  percent_sequence_before_anomaly, \n",
    "  percent_sequence_after_anomaly, \n",
    "  normal_freq, \n",
    "  normal_ampl, \n",
    "  normal_noise_noise_scale):\n",
    "  seq_len_before_anomaly = int(seq_len * percent_sequence_before_anomaly / 100.0)\n",
    "  seq_len_after_anomaly = int(seq_len * percent_sequence_after_anomaly / 100.0)\n",
    "  seq_len_anomaly = seq_len - seq_len_before_anomaly - seq_len_after_anomaly\n",
    "\n",
    "  # Anomalous parameters\n",
    "  anomalous_ampl_multipler_min = 8.0\n",
    "  anomalous_ampl_multipler_max = 20.0\n",
    "\n",
    "  if simple_data_genertator == True:\n",
    "    sequence_with_anomaly = np.stack(\n",
    "      arrays = [np.sin(np.arange(0, seq_len) * normal_freq) * normal_ampl \n",
    "          for _ in range(number_of_sequences)], axis = 0)\n",
    "  else:\n",
    "    sequence_with_anomaly = create_time_series_normal(\n",
    "      number_of_sequences, seq_len, normal_freq, normal_ampl, normal_noise_noise_scale)\n",
    "  sequence_with_anomaly[:, seq_len_before_anomaly:seq_len_before_anomaly + seq_len_anomaly] *= \\\n",
    "    ((anomalous_ampl_multipler_max - anomalous_ampl_multipler_min) * \\\n",
    "     np.random.random_sample([number_of_sequences, seq_len_anomaly]) + anomalous_ampl_multipler_min) * \\\n",
    "    (np.random.randint(2, size = [number_of_sequences, seq_len_anomaly]) * -2 + 1)\n",
    "  \n",
    "  return sequence_with_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_normal_parameters = create_time_series_normal_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/seaborn/timeseries.py:183: UserWarning: The `tsplot` function is deprecated and will be removed in a future release. Please update your code to use the new `lineplot` function.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "for i in range(0, 5):\n",
    "  sns.tsplot(create_time_series_normal(1, 10, test_normal_parameters[\"normal_freq\"], test_normal_parameters[\"normal_ampl\"], test_normal_parameters[\"normal_noise_noise_scale\"]).reshape(-1), color=flatui[i%len(flatui)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "for i in range(0, 10):\n",
    "  sns.tsplot(create_time_series_with_anomaly(1, 10, percent_sequence_before_anomaly, percent_sequence_after_anomaly, test_normal_parameters[\"normal_freq\"], test_normal_parameters[\"normal_ampl\"], test_normal_parameters[\"normal_noise_noise_scale\"]).reshape(-1), color=flatui[i%len(flatui)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_normal_sequences = 64000\n",
    "\n",
    "number_of_validation_normal_1_sequences = 6400\n",
    "number_of_validation_normal_2_sequences = 6400\n",
    "number_of_validation_anomalous_sequences = 6400\n",
    "\n",
    "number_of_test_normal_sequences = 6400\n",
    "number_of_test_anomalous_sequences = 6400\n",
    "\n",
    "seq_len = 30\n",
    "number_of_tags = 5\n",
    "tag_columns = [\"tag_{0}\".format(tag) for tag in range(0, number_of_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'normal_ampl': 1.778972872563362,\n",
       "  'normal_freq': 1.4417891446257471,\n",
       "  'normal_noise_noise_scale': 1.0},\n",
       " {'normal_ampl': 1.4870514117516036,\n",
       "  'normal_freq': 1.4408032040795131,\n",
       "  'normal_noise_noise_scale': 1.0},\n",
       " {'normal_ampl': 1.8221932704880583,\n",
       "  'normal_freq': 1.248934347464297,\n",
       "  'normal_noise_noise_scale': 1.0},\n",
       " {'normal_ampl': 1.2224228582084027,\n",
       "  'normal_freq': 1.3008776720890425,\n",
       "  'normal_noise_noise_scale': 1.0},\n",
       " {'normal_ampl': 1.88655151282773,\n",
       "  'normal_freq': 1.660280650102402,\n",
       "  'normal_noise_noise_scale': 1.0}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_data_list = [create_time_series_normal_parameters() \n",
    "                 for tag in range(0, number_of_tags)]\n",
    "tag_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_normal_sequences_array.shape = \n",
      "(64000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create training set using normal sequences\n",
    "training_normal_sequences_list = [create_time_series_normal(\n",
    "  number_of_training_normal_sequences, seq_len, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) \n",
    "                                  for tag in tag_data_list]\n",
    "training_normal_sequences_array = np.stack(\n",
    "  arrays = list(map(\n",
    "    lambda i: np.stack(\n",
    "      arrays = list(map(\n",
    "        lambda j: np.array2string(\n",
    "          a = training_normal_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), \n",
    "        np.arange(0, number_of_training_normal_sequences))), \n",
    "      axis = 0), \n",
    "    np.arange(0, number_of_tags))), \n",
    "  axis = 1)\n",
    "np.random.shuffle(training_normal_sequences_array)\n",
    "print(\"training_normal_sequences_array.shape = \\n{}\".format(training_normal_sequences_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_normal_1_sequences_array.shape = \n",
      "(6400, 5)\n",
      "validation_normal_2_sequences_array.shape = \n",
      "(6400, 5)\n",
      "validation_anomalous_sequences_array.shape = \n",
      "(6400, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create validation sets\n",
    "# Create set vn1 of normal sequences which will be used for early stopping during training as well as using the error vectors to learn mu and sigma for mahalanobis distance\n",
    "validation_normal_1_sequences_list = [create_time_series_normal(\n",
    "  number_of_validation_normal_1_sequences, seq_len, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) \n",
    "                                      for tag in tag_data_list]\n",
    "validation_normal_1_sequences_array = np.stack(\n",
    "  arrays = list(map(\n",
    "    lambda i: np.stack(\n",
    "      arrays = list(map(\n",
    "        lambda j: np.array2string(\n",
    "          a = validation_normal_1_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), \n",
    "        np.arange(0, number_of_validation_normal_1_sequences))), \n",
    "      axis = 0), \n",
    "    np.arange(0, number_of_tags))), \n",
    "  axis = 1)\n",
    "print(\"validation_normal_1_sequences_array.shape = \\n{}\".format(validation_normal_1_sequences_array.shape))\n",
    "\n",
    "# Create set vn2 of normal sequences which will be used for tuning the anomaly thresholds\n",
    "validation_normal_2_sequences_list = [create_time_series_normal(\n",
    "  number_of_validation_normal_2_sequences, seq_len, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) \n",
    "                                      for tag in tag_data_list]\n",
    "validation_normal_2_sequences_array = np.stack(\n",
    "  arrays = list(map(lambda i: np.stack(arrays = list(map(\n",
    "    lambda j: np.array2string(\n",
    "      a = validation_normal_2_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), \n",
    "    np.arange(0, number_of_validation_normal_2_sequences))), \n",
    "                                       axis = 0), \n",
    "                    np.arange(0, number_of_tags))), \n",
    "  axis = 1)\n",
    "print(\"validation_normal_2_sequences_array.shape = \\n{}\".format(validation_normal_2_sequences_array.shape))\n",
    "\n",
    "# Create set va of anomalous sequences which will be used for tuning the anomaly thresholds\n",
    "validation_anomalous_sequences_list = [create_time_series_with_anomaly(\n",
    "  number_of_validation_anomalous_sequences, seq_len, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) \n",
    "                                       for tag in tag_data_list]\n",
    "validation_anomalous_sequences_array = np.stack(\n",
    "  arrays = list(map(\n",
    "    lambda i: np.stack(arrays = list(map(\n",
    "      lambda j: np.array2string(\n",
    "        a = validation_anomalous_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), \n",
    "      np.arange(0, number_of_validation_anomalous_sequences))), \n",
    "                       axis = 0), \n",
    "    np.arange(0, number_of_tags))), \n",
    "  axis = 1)\n",
    "print(\"validation_anomalous_sequences_array.shape = \\n{}\".format(validation_anomalous_sequences_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_sequences_array.shape = \n",
      "(6400, 5)\n",
      "test_anomalous_sequences_array.shape = \n",
      "(6400, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create test sets\n",
    "# Create set tn of normal sequences which will be used for testing model\n",
    "test_normal_sequences_list = [create_time_series_normal(\n",
    "  number_of_test_normal_sequences, seq_len, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) \n",
    "                              for tag in tag_data_list]\n",
    "test_normal_sequences_array = np.stack(\n",
    "  arrays = list(map(\n",
    "    lambda i: np.stack(\n",
    "      arrays = list(map(\n",
    "        lambda j: np.array2string(\n",
    "          a = test_normal_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), \n",
    "        np.arange(0, number_of_test_normal_sequences))), \n",
    "      axis = 0), \n",
    "    np.arange(0, number_of_tags))), \n",
    "  axis = 1)\n",
    "print(\"test_normal_sequences_array.shape = \\n{}\".format(test_normal_sequences_array.shape))\n",
    "\n",
    "# Create set ta of anomalous sequences which will be used for testing model\n",
    "test_anomalous_sequences_list = [create_time_series_with_anomaly(\n",
    "  number_of_test_anomalous_sequences, seq_len, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) \n",
    "                                 for tag in tag_data_list]\n",
    "test_anomalous_sequences_array = np.stack(\n",
    "  arrays = list(map(\n",
    "    lambda i: np.stack(\n",
    "      arrays = list(map(\n",
    "        lambda j: np.array2string(\n",
    "          a = test_anomalous_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), \n",
    "        np.arange(0, number_of_test_anomalous_sequences))), \n",
    "      axis = 0), \n",
    "    np.arange(0, number_of_tags))), \n",
    "  axis = 1)\n",
    "print(\"test_anomalous_sequences_array.shape = \\n{}\".format(test_anomalous_sequences_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled_validation_mixed_sequences_array.shape = \n",
      "(12800, 6)\n",
      "labeled_test_mixed_sequences_array.shape = \n",
      "(12800, 6)\n"
     ]
    }
   ],
   "source": [
    "# Combine vn2 and va sets for tuning anomaly thresholds\n",
    "labeled_validation_normal_2_sequences_array = np.concatenate(\n",
    "  seq = [validation_normal_2_sequences_array, \n",
    "         np.zeros(shape = [validation_normal_2_sequences_array.shape[0], 1], dtype = np.int64)], \n",
    "  axis = 1)\n",
    "labeled_validation_anomalous_sequences_array = np.concatenate(\n",
    "  seq = [validation_anomalous_sequences_array, \n",
    "         np.ones(shape = [validation_anomalous_sequences_array.shape[0], 1], dtype = np.int64)], \n",
    "  axis = 1)\n",
    "labeled_validation_mixed_sequences_array = np.concatenate(\n",
    "  seq = [labeled_validation_normal_2_sequences_array, \n",
    "         labeled_validation_anomalous_sequences_array], \n",
    "  axis = 0)\n",
    "np.random.shuffle(labeled_validation_mixed_sequences_array)\n",
    "print(\"labeled_validation_mixed_sequences_array.shape = \\n{}\".format(labeled_validation_mixed_sequences_array.shape))\n",
    "\n",
    "# Combine tn and ta sets for testing model\n",
    "labeled_test_normal_sequences_array = np.concatenate(\n",
    "  seq = [test_normal_sequences_array, \n",
    "         np.zeros(shape = [test_normal_sequences_array.shape[0], 1], dtype = np.int64)], \n",
    "  axis = 1)\n",
    "labled_test_anomalous_sequences_array = np.concatenate(\n",
    "  seq = [test_anomalous_sequences_array, \n",
    "         np.ones(shape = [test_anomalous_sequences_array.shape[0], 1], dtype = np.int64)], \n",
    "  axis = 1)\n",
    "labeled_test_mixed_sequences_array = np.concatenate(\n",
    "  seq = [labeled_test_normal_sequences_array, \n",
    "         labled_test_anomalous_sequences_array], \n",
    "  axis = 0)\n",
    "np.random.shuffle(labeled_test_mixed_sequences_array)\n",
    "print(\"labeled_test_mixed_sequences_array.shape = \\n{}\".format(labeled_test_mixed_sequences_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(fname = \"data/training_normal_sequences.csv\", \n",
    "           X = training_normal_sequences_array, \n",
    "           fmt = '%s', \n",
    "           delimiter = \";\")\n",
    "\n",
    "np.savetxt(fname = \"data/validation_normal_1_sequences.csv\", \n",
    "           X = validation_normal_1_sequences_array, \n",
    "           fmt = '%s', \n",
    "           delimiter = \";\")\n",
    "\n",
    "np.savetxt(fname = \"data/labeled_validation_mixed_sequences.csv\", \n",
    "           X = labeled_validation_mixed_sequences_array, \n",
    "           fmt = '%s', \n",
    "           delimiter = \";\")\n",
    "\n",
    "np.savetxt(fname = \"data/labeled_test_mixed_sequences.csv\", \n",
    "           X = labeled_test_mixed_sequences_array, \n",
    "           fmt = '%s', \n",
    "           delimiter = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11820002,1.88482704,1.41632499,-1.02183151,-0.21532498,2.00405017,2.24194393,-0.23334654,-0.84551505,0.79562927,2.12968145,0.30633567,-1.76779828,0.67659071,2.11095124,1.42272425,-0.58168264,-0.76756326,1.71281337,2.17522933,-0.45938616,-1.31523576,1.23531058,2.21053737,0.39571181,-1.21633858,-0.24531372,2.50412967,1.09628421,-1.14938518;0.18382153,1.77531425,0.82007252,-1.11564749,0.11727255,1.49870908,1.80390502,-0.14014266,-0.74051729,0.73443898,2.08159382,0.31456765,-1.41512407,0.8200266,2.1231802,1.04635518,-0.85772756,-0.12716312,1.70626393,1.65263624,-0.0179455,-0.46262927,0.81346933,1.59434239,0.74338835,-0.83331125,0.27052981,1.43240741,0.82889839,-1.09526816;0.72933327,2.13671962,1.68144825,-0.38010543,-1.50737836,-0.05174894,2.32786148,1.73092373,-0.74301199,-1.3947686,-0.10073033,2.10595226,1.72896754,-0.58305614,-1.71009025,0.10506023,2.14932839,2.06817019,-0.0992871,-1.28203463,0.59162493,1.91585819,2.00213707,0.01392596,-0.88223442,0.55311989,1.99198025,1.97496052,0.10218877,-1.08919233;0.28930885,2.00027565,0.96609301,-0.67577958,-0.55815001,0.63471686,1.24704129,0.70396195,-0.38268581,-0.08636247,0.91408872,1.32158673,0.85772953,-0.17027583,-0.50937538,1.20645886,1.55820024,0.24421484,-0.79509037,0.01419417,1.65717373,1.82482616,0.55763234,-0.81229255,0.28200668,1.11755897,1.50359116,0.26135911,-0.45965764,0.42610418;0.09462625,2.11879405,0.55091793,-1.10444785,1.29677115,2.0286215,-0.68318779,-1.37155879,1.74124605,2.22241235,-0.82213204,-0.7267117,2.2021394,1.33023576,-1.15785494,0.40609775,2.42611319,0.17068174,-0.9124492,0.2658295,2.47605616,0.15340102,-1.16437949,1.62036098,2.14763435,-1.08798358,-0.99326565,1.67641481,1.74368876,-1.08202372\n",
      "0.37351041,2.26499526,1.26728024,-1.00380559,-0.24133809,1.68490791,1.88177943,-0.30854828,-0.80951339,1.05602729,1.94807859,0.05064848,-0.79139426,-0.10773623,2.58526753,0.65421786,-1.23200389,-0.33556365,1.81665718,2.33378165,-0.38376454,-0.72789893,1.41444868,2.297531,0.02502792,-0.87327209,-0.17892897,2.03401193,1.43687816,-1.29000134;0.62055623,2.26919424,0.39483105,-1.10685577,-0.06592311,1.55296538,1.1854453,-0.29611734,-0.99398408,1.12563162,2.42271115,-0.15136397,-0.73598482,0.54752383,1.84088927,0.94853822,-0.99512313,0.04480077,1.86032982,1.21944547,0.21930713,-0.39133123,0.78362554,2.44471536,0.77979566,-0.99067972,0.37744439,1.6387848,0.87444738,-0.28473165;0.86715277,2.40479406,1.48187372,-0.72156055,-1.21066593,0.36739631,1.9513959,1.16088824,-0.22137598,-1.25721586,0.25663594,2.30368989,2.12766093,-0.03740994,-0.94873019,0.62520561,1.91757196,2.19487226,0.08169721,-0.97549257,0.16787351,1.98673379,1.38342611,-0.37549123,-1.01816586,0.16081413,1.59733625,1.49711596,-0.43710406,-1.08096759;0.21947293,1.84632221,1.49465711,-0.24383435,-0.77862798,0.76551096,1.84174844,0.803581,-0.94690041,-0.49497077,1.24836094,1.74268902,1.10520497,-0.7293229,-0.41032372,1.21479265,1.58228777,0.39568436,-0.54044789,0.22868043,1.11589019,1.44637337,0.35421727,-0.41767089,0.72297358,1.12120325,1.18407871,0.16570669,-0.2630825,0.39774496;0.98521306,2.34010404,0.39493095,-1.18249413,1.56657059,2.3677376,-0.74142542,-0.57463775,1.60932752,1.85872739,-0.7781468,-0.09499221,2.56358247,0.82331412,-1.37971698,-0.39113158,2.86291273,0.40993573,-1.85871543,0.38526805,2.75432509,-0.56215387,-0.82519373,1.11694282,2.19750212,-0.16810926,-0.77516823,1.97676696,1.43772875,-0.70578129\n",
      "0.10701877,2.31835726,1.36815897,-1.36531573,0.11260164,1.9709501,2.00781061,-0.70341994,-1.52304148,1.32946468,2.12237784,-0.20198946,-1.75326765,-0.12599342,2.327545,1.08929874,-1.04119824,-0.61991653,1.99024123,2.1372361,-0.60097821,-0.97505972,0.7104116,2.63378409,-0.03831735,-1.7542943,-0.05333567,2.38783602,1.36741724,-1.36357198;0.14745585,1.84110344,1.18261513,-0.58825731,0.2563761,1.2833358,1.43190602,-0.57557854,-1.01421815,1.42171771,1.55558526,-0.02233174,-0.89105358,0.1574847,1.85487182,0.91706218,-0.79242678,-0.74687377,1.30379318,1.35065449,-0.24855616,-0.78056735,1.0207875,2.19792831,0.06683788,-1.12792445,-0.11383521,1.98668418,1.55718756,-0.63827089;0.42452523,2.24354143,1.68143138,-0.34821194,-0.80811062,0.87170218,2.08333899,1.64067205,-0.79007467,-1.21101039,0.61795349,2.4819952,1.59006737,-0.48571446,-0.85201973,0.35547112,2.42739336,1.55135267,-0.06515015,-1.00017278,-0.11355838,1.90275207,1.91720969,-0.64959235,-0.84629384,0.35042738,1.79617311,2.26968748,-0.58385718,-1.64655405;0.59781226,1.94720735,1.02014545,-0.62857629,-0.60652873,0.84077554,1.73041779,1.27828221,-0.43318156,-0.61744549,0.55711995,1.75328995,0.72000665,-0.23381801,-0.06311997,1.73410448,1.9980637,0.74817808,-0.27495583,0.38185925,1.68837397,1.04354166,-0.26060298,-0.86475605,-0.10924451,1.67793936,1.79793421,-0.23551191,-0.83935499,0.92036211;0.51978787,2.06681549,0.39985556,-0.91397249,0.70019757,2.14605619,-0.5991363,-1.10876724,1.61328243,2.03021818,-1.27441092,-0.14149282,2.52393575,0.8432118,-1.58233323,0.20953558,2.8042902,0.16642945,-0.91855521,0.97494489,2.44856323,-0.14893021,-1.38067748,0.96281063,2.16527142,-0.27686125,-0.53017797,2.36183636,1.80328348,-1.43443761\n"
     ]
    }
   ],
   "source": [
    "!head -3 data/training_normal_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63347167,2.45427723,1.40786296,-1.40426412,-0.66251296,1.63547824,2.00230881,-0.44770993,-0.91651791,1.19828131,2.43188968,0.39724988,-1.01982264,0.78409578,2.6005846,0.7452633,-1.26955776,-0.15225321,2.21921078,2.3081579,-0.75533229,-1.00472412,0.67198836,2.09810952,0.76300896,-1.08360963,-0.06907406,2.52956977,1.15341058,-0.63884795;0.44663338,2.09372316,1.10814223,-1.00126131,-0.22434752,2.11309892,1.76336151,-0.66774022,-0.89187416,0.77022549,1.45635606,-0.10301715,-0.48756438,0.59058198,1.59858179,1.54145325,-0.5384744,-0.67485301,1.54702258,1.47039464,-0.0191825,-1.00175524,1.12341402,2.40569977,0.91503921,-0.75734909,0.08714054,2.2078818,0.76195713,-0.67507847;0.30237686,2.2900409,1.25957209,-0.07573345,-1.04630332,0.16721076,2.19624901,1.87758654,-0.96575257,-0.84545299,0.66907835,1.92290837,1.51246029,-0.23649955,-1.47391163,0.48766045,1.84447981,1.82124654,-0.75869173,-1.17270969,0.23708638,2.19683738,2.18407906,0.09000157,-1.67402083,0.02462948,1.69193644,2.29140097,0.21059303,-0.91755589;0.8317677,1.56596551,1.46237663,-0.13562766,-0.14718871,1.14904277,1.99746491,0.63665244,-0.44665831,-0.29722291,1.29886349,1.50431775,0.33638335,-0.36093781,-0.25094989,1.10041824,1.79338106,-0.00728263,-0.64138586,-0.4483994,1.19375189,1.59487666,0.12522325,-0.29082833,-0.18731984,1.14967743,1.68396685,-0.42968702,-0.26074941,0.96514719;0.70955909,2.32121858,-0.20265909,-1.74099563,1.40981497,2.31272286,-0.81267493,-0.53038922,1.70664268,2.1979952,-0.82173084,-0.95463939,2.25883854,1.73269598,-1.28036531,-0.09054001,2.15039741,1.08244437,-0.94138284,0.54962196,2.03334009,-0.41634084,-1.14029573,0.9498978,1.94126904,-1.11789233,-0.40876155,2.0559034,1.55796077,-1.34387304\n",
      "0.78816487,2.57689076,0.59404364,-0.99253573,-0.53164089,2.28739213,1.50923728,-0.2636639,-1.25950435,1.2223597,1.83356297,-0.20521606,-1.51339207,-0.14658291,2.64679932,0.84582623,-1.11205307,-0.64590137,1.75976451,2.01176948,-0.32469074,-1.60745331,1.38711167,1.76779736,0.34760749,-0.95336186,0.43132174,2.45506504,1.53347825,-0.86808181;0.87311151,2.23588651,1.27240186,-1.1286191,-0.32823914,2.1607913,1.27521962,-0.68125811,-0.93035976,1.49740074,2.10800364,0.46331537,-0.83790672,-0.11946531,1.45807499,0.57824602,-0.59823104,-0.31772458,1.70265849,1.49121663,-0.24404121,-0.57900687,0.98420554,2.28163039,0.81858541,-1.23991544,0.17538472,1.89637863,1.37268847,-1.11306761;0.5990498,2.14524887,1.3832012,-0.29783393,-0.80435173,0.06687983,2.57084256,1.32707768,-0.21372789,-1.32478277,-0.06930261,1.77375725,2.03138421,-0.0790681,-1.50778321,-0.12150173,2.10801798,1.9246979,-0.56865201,-1.46182966,0.66294846,2.25911131,1.3774363,-0.06127816,-0.91039777,0.47698643,2.52150257,1.46605436,0.04828565,-1.09057535;0.39163945,1.36559608,1.622511,-0.35456418,-1.0356627,0.74804881,1.87138793,0.63709558,-0.51037147,-0.22237961,0.82589859,2.19296681,1.03628574,-0.80359107,0.18061816,1.43269484,1.98154225,0.18373623,-0.99259395,-0.03691613,0.94737694,1.0983239,-0.05182035,-0.90444021,0.55411879,1.20886314,1.19456347,0.17997885,-0.78953473,0.41871277;0.20112471,2.15721107,0.12143397,-0.86783826,0.8949632,1.8705579,-0.70940999,-0.66642051,2.02020909,1.7747535,-1.35396332,-0.05968182,2.64494827,1.15385815,-1.4404452,0.35640333,2.1057549,0.50264355,-0.91797883,0.97058762,1.88724204,0.17552682,-1.04596486,0.9389696,2.48756695,-0.96703315,-0.69036706,1.84876739,1.23539531,-1.28829594\n",
      "0.35145976,2.13735524,1.02674989,-0.90778965,-0.05447445,2.23382613,1.79136144,-0.4197466,-0.85677311,1.32610619,2.60204725,0.70239577,-1.19293617,0.11161712,2.03151269,1.22493865,-1.31444112,-0.74048419,1.77708206,1.78673305,0.03860852,-1.31739262,1.39909891,2.27513673,0.18328037,-1.13479394,0.42250968,2.24943908,0.83876367,-0.52496908;0.69800365,1.81416725,1.31780479,-0.58661322,0.02852533,1.57445426,1.36378184,0.03077371,-1.10680665,0.60815843,2.24297156,-0.15205428,-1.29743361,0.81190269,1.58183944,1.32274413,-1.04995144,-0.45647334,1.45325352,1.66826012,0.00635064,-1.17694394,0.44584953,2.06491817,0.28672781,-0.735653,0.04910837,2.03363963,1.16939289,-0.66878994;0.41018529,2.01589401,1.82544812,-0.74691377,-1.11149756,0.62191545,1.92312666,1.17355519,-0.78944935,-0.82242457,0.45029012,2.46005597,1.34087849,-0.42932457,-1.28001036,0.62267626,2.49522072,1.87515265,-0.47905357,-1.7065506,-0.26214114,1.69605171,1.3552992,-0.25768488,-1.53558042,0.10317337,1.6436602,1.3577963,-0.51687615,-1.32183725;0.60120701,1.89761848,1.52949505,-0.75548812,-0.1654033,0.38191765,1.97368592,1.03659382,-0.4530184,-0.56695396,1.02969595,1.54226289,0.59251285,-1.04186904,-0.68927593,1.26265487,1.44046714,-0.05948034,-0.51243508,-0.24324325,1.5538878,1.03812668,0.05080549,-0.37051772,0.04380985,1.53410472,1.55504343,-0.41027217,-0.90749346,0.07820974;0.0196163,2.44683796,-0.28594048,-0.965395,0.85050076,2.12410488,-0.50021431,-0.83015416,1.71194111,1.81432863,-0.91845626,-0.14312323,1.67728569,0.9472233,-1.41671873,0.00801583,1.99152915,0.74114408,-1.31162519,0.66449152,2.76299629,0.25993754,-1.19611137,0.91082733,2.00336062,-0.37255263,-1.16175548,1.58059083,1.74929324,-0.95099111\n"
     ]
    }
   ],
   "source": [
    "!head -3 data/validation_normal_1_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26811025,2.50027179,1.01833031,-0.92524951,-0.67698776,1.57049967,1.70130292,-0.22372209,-0.82317685,1.51601039,2.00979593,-0.09555506,-1.74840777,0.32198378,2.50272099,0.88055993,-0.67982987,-0.62124956,2.25146884,2.18792039,-0.33021312,8.24285697,9.40233475,-30.184897,-16.10511008,-18.53804545,-6.04998052,45.85966958,-19.07836118,13.97450729;0.03788288,2.23032734,0.61078838,-1.05687079,-0.5265466,1.94422789,1.26309426,-0.13955816,-0.67658178,0.62857689,1.92441221,0.32325632,-0.59366724,0.52183015,1.55067156,1.02940051,-0.76589503,-0.34817361,1.77060969,1.27790516,-0.3461714,9.84017221,-7.52018099,-14.23614688,-11.3018325,10.74861767,-1.64161545,27.91358382,-12.22922672,9.2894981;1.92689501e-01,2.02356347e+00,1.21723186e+00,-2.40127431e-01,-8.59787670e-01,1.53201275e-01,2.62386028e+00,1.34395024e+00,-3.90476117e-01,-1.17253982e+00,3.33350561e-01,2.65324293e+00,1.78277388e+00,-2.69234367e-02,-1.09162127e+00,2.99615170e-01,2.22441333e+00,1.94883242e+00,-6.29070087e-03,-1.34904841e+00,6.63420937e-01,-3.34085286e+01,2.01116303e+01,-2.95469192e+00,-1.25056705e+01,-7.97049186e+00,-1.84496598e+01,1.34292847e+01,-8.80511245e-01,-1.62618769e+01;0.18624791,1.50890622,1.62515919,-0.28258345,-0.13731504,0.31775118,1.34824518,0.9795557,-0.1017194,0.04385926,1.51436072,1.32377472,0.51149248,-1.07065049,-0.62472008,1.58252922,1.7182727,-0.03724006,-1.11094293,0.40589989,1.2883821,-30.36848865,-4.06001506,-14.45340067,-2.71362515,-23.35490648,-25.01242915,5.11226149,-17.30886388,6.66170006;2.08064522e-02,2.44003015e+00,6.18112177e-01,-1.76423977e+00,1.48714894e+00,2.58282314e+00,-2.14165437e-02,-1.00775115e+00,2.00779031e+00,1.81050613e+00,-1.17014428e+00,-5.80620936e-02,1.84526084e+00,1.53248849e+00,-9.64997146e-01,-3.94917776e-01,2.73834636e+00,5.31625870e-01,-1.41486485e+00,5.12344331e-01,2.11655568e+00,3.34274834e+00,-1.33473043e+01,-2.08804790e+01,-3.36253976e+01,5.93973078e+00,-4.95955486e+00,-3.00976784e+01,2.08744802e+01,2.63975857e+01;1\n",
      "0.98681266,1.87309693,0.81914789,-1.14325583,-0.10338238,2.00135836,2.19855281,-0.61605901,-1.254269,0.94064997,2.41911696,-0.19114325,-1.12214818,0.26600686,2.41079764,0.69552912,-1.39964876,-0.99029324,1.90912616,2.03224673,-0.32236786,-0.73376535,0.77823847,2.14089519,0.56852137,-1.06759419,0.20572196,1.92911859,1.54805079,-0.90777511;0.7230913,1.67430985,0.86281015,-1.20997536,0.18989811,1.43079701,1.78489521,-0.71884848,-1.26857539,1.09639709,2.0184406,-0.04046995,-1.42567364,0.1056667,2.08062653,0.88977608,-0.57789832,-0.84146203,1.10187175,1.88950331,-0.0273484,-1.26437465,0.42411338,1.9888766,0.90380814,-0.93950733,0.34833383,1.70358599,1.07385973,-0.33481776;0.86901761,2.23016029,1.79071291,-0.98796703,-1.53229825,0.78282278,2.05276852,1.95633799,-0.22361973,-1.32514933,0.70660624,2.05399771,2.03150665,-0.3945807,-1.52236816,-0.11906092,2.47057186,1.52137462,0.00970397,-0.94789368,0.2239537,2.25576736,1.66260655,-0.68898185,-0.87587725,0.05988201,1.7748285,1.83434697,-0.12303241,-1.05692522;0.92180944,1.41607108,0.76521808,-0.70598953,-0.73960959,0.9117621,1.32324326,0.44915756,-0.1092808,-0.76704886,0.85987349,1.70122173,0.51461247,-1.00737122,-0.16900038,1.13815126,2.04479431,0.46533089,-1.06804535,0.2842891,1.2294481,1.77676125,-0.01218305,-0.50110217,0.06642312,1.14392914,1.35628954,-0.16430311,-0.42438334,0.17096419;0.63918033,2.07779022,0.43891684,-1.3081099,1.30541541,2.43765085,-0.28460413,-1.17359918,1.98170579,1.82996636,-0.50713905,-0.675398,2.57964549,1.04532166,-1.56228454,-0.26041224,2.60880345,0.82663681,-1.86267486,0.99419621,1.91704139,-0.33649893,-1.43761145,1.69886254,1.73122454,-0.19680803,-0.92482782,1.53133952,1.82816769,-0.87814578;0\n",
      "0.50151595,2.32502971,0.64178836,-0.82689569,-0.25681235,1.43583475,1.31151041,-1.05187358,-1.28578787,1.0377023,2.29329664,-0.12902204,-1.33264426,0.55935106,2.10413546,1.19374621,-1.20071052,-0.08627268,1.5980747,2.25807006,-0.07473499,-1.43845271,0.89120258,2.20820658,-0.044018,-1.28153943,0.3429125,1.87177949,1.17510034,-1.27792378;0.87423057,2.41797366,0.70291754,-1.10465974,0.1880088,1.73699111,1.15428408,-0.20132358,-0.66900352,0.66406433,2.13662563,0.1088461,-0.80039271,0.31783928,1.74028504,0.85495089,-0.36579369,-0.24981062,1.83373905,1.50769147,0.03668313,-0.66341433,0.85963665,1.58996325,0.10707799,-0.70928891,0.05155036,1.83029322,1.69502291,-0.7132957;0.63824815,2.33900211,1.70231141,-0.28493815,-1.04746086,0.06779335,2.13638692,1.43872821,-0.62758521,-0.88885951,-0.09448777,1.98605679,1.2899991,-0.42064277,-0.84771605,0.4317382,2.02471938,1.96118792,-0.55836833,-1.36499992,0.51043129,2.06556679,2.18423947,-0.38756818,-1.44457024,0.64615367,1.94843866,1.67919923,-0.38248214,-1.02234051;0.1670683,2.01677664,1.05732005,-0.69672263,-0.09547731,0.93039047,1.36308586,0.78393334,-0.46794199,-0.7001272,0.83617286,1.68068288,0.18487245,-0.84357904,-0.34569416,1.22438026,1.14705459,0.33225279,-1.05902398,-0.48696368,1.36398427,1.85099915,-0.39318879,-0.4676125,0.28836984,1.165949,1.68367452,0.14966288,-0.92838395,0.89984708;0.24147712,2.27552569,0.33842239,-1.70792738,1.48069807,2.12525576,-0.34209459,-1.28458787,1.91531222,2.24389318,-0.70083205,-0.50706251,2.26184416,1.40484294,-1.46082606,0.16982788,2.25204721,0.20293011,-1.15796549,0.63601782,2.1369906,-0.27728149,-1.16716057,1.69967442,2.03523588,-0.36999299,-0.64238031,1.88690635,1.48920395,-1.04419528;0\n"
     ]
    }
   ],
   "source": [
    "!head -3 data/labeled_validation_mixed_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56895215,2.0018052,1.2249,-1.07187086,-0.30388412,2.18232897,1.45916312,-0.75930385,-0.68322231,1.45937461,1.72802532,0.70617995,-0.86114675,0.71675355,2.44877348,1.17742741,-1.01852682,-0.6395642,2.1523235,2.31418228,-0.3830392,-14.95512042,-11.06455875,32.41062859,-8.49482075,-31.7362507,-0.81061033,27.93859882,-28.72655205,9.81776072;0.8817524,1.83997394,0.66556157,-0.98999201,-0.07161556,1.27736729,1.77092118,-0.29542186,-1.03080431,1.03721686,1.7155006,-0.0764367,-0.82692211,0.66513228,1.70585875,1.13777627,-0.84239697,-0.14617438,1.63304094,1.66280439,-0.34871063,5.11015507,-8.16719047,15.45004105,-15.08744831,5.88473691,-5.24807277,16.66477002,-13.23406224,-10.15132636;0.9674892,2.66418674,1.8452546,-0.27899184,-1.61859882,0.50011809,2.02590232,1.34699006,-0.1122609,-1.37842385,0.04736702,2.41141345,1.82043601,0.02581422,-1.74482867,-0.13517803,2.48169344,1.50377808,-0.32816617,-1.55489357,0.42129007,18.3440187,-19.22860765,2.05803048,12.28694445,2.08313033,20.07401069,-18.74792599,5.13667118,-22.15314969;4.87722388e-01,1.52750381e+00,1.51654143e+00,-2.58234200e-01,-4.43481283e-01,3.07721506e-01,1.34446514e+00,7.99298783e-01,-3.54361293e-01,-7.28842437e-01,1.04930092e+00,1.88345531e+00,1.00565078e+00,-7.55338020e-01,-3.77751706e-01,1.27188230e+00,1.19415592e+00,1.72086328e-02,-3.98598943e-01,1.76399894e-02,1.82430941e+00,-2.60536504e+01,-2.22877604e+00,4.59773552e+00,1.00110597e+01,1.45348602e+01,2.24615286e+01,-1.77325191e+00,1.00750885e+01,-9.96236370e+00;0.10827723,2.33067708,0.65092288,-1.7172468,1.0075206,1.90127209,-0.36519373,-0.84380298,1.93185973,1.7419125,-1.40668646,-0.43652464,2.08536483,1.17532792,-1.25406343,-0.39256008,2.11751317,0.19848673,-1.0688665,1.02460388,2.67669275,3.88789773,-18.59004308,-20.01507224,-27.86633111,4.99371575,11.22474659,-23.63015758,-25.44682904,-10.29554958;1\n",
      "0.39172276,2.2532194,0.9443752,-1.44014507,-0.25500637,1.74687845,1.76908628,-0.18397222,-1.49060651,0.97159658,2.3320496,0.37416946,-1.20174809,0.48783923,2.39913776,1.0644531,-0.68891691,-0.72574303,1.81171579,2.30931968,-0.84932868,11.70623513,-8.02323204,46.82702825,-0.060159,18.42604412,1.04607746,29.64273343,30.48638618,13.46116688;0.3114145,1.74974134,0.75868422,-1.3449364,0.19223994,1.51313272,1.58860024,-0.85832731,-0.93750492,0.93941535,1.97160568,0.19898144,-0.8539687,0.1499638,1.56348945,0.98538131,-1.21184083,-0.23818336,1.34226756,1.64532096,-0.55384461,-11.32941088,8.66467817,-23.61522254,-10.75057578,6.2472422,5.73436013,30.1111359,14.34289241,13.00383579;6.09025357e-01,1.93764619e+00,1.89287296e+00,-4.75683657e-01,-1.19344389e+00,2.25955347e-01,2.09375487e+00,2.12186274e+00,-1.02500276e-02,-1.60657104e+00,6.39390912e-01,1.79090254e+00,1.88392102e+00,5.23841793e-02,-1.45421291e+00,-1.58054537e-02,1.79557870e+00,1.91738664e+00,8.90518187e-02,-8.02148204e-01,3.90971200e-01,-3.76603896e+01,-1.60450490e+01,5.72099918e+00,1.73984843e+01,7.54887450e-01,3.11664951e+01,-3.19946545e+01,-3.80603433e+00,-2.59848292e+01;0.20703776,1.63539411,1.46175019,-0.15474465,-0.29049847,0.67447491,1.33024971,0.52812729,-0.77517429,0.05276119,1.09573544,2.0952622,0.46565018,-0.93595757,-0.04361328,1.65088124,1.38907054,0.41470207,-1.19171596,0.19247518,1.05891403,26.16275481,6.24980656,12.05851559,1.4301751,-12.41001118,-18.02130686,-8.64843953,-8.2429083,-8.47477973;0.47930985,2.65768402,-0.28058719,-1.59234011,1.42903809,1.74160635,-0.91018419,-0.88236209,1.42281239,2.14921166,-0.85371183,-0.382842,1.87848192,1.16797547,-0.85033405,0.0893317,2.00057538,0.14723972,-1.43089782,1.00227233,2.77340252,0.29149157,-10.549842,21.09876142,35.59754913,20.54760448,8.91284667,27.77181419,27.54552785,6.95126373;1\n",
      "7.89394837e-02,2.45865560e+00,1.16530477e+00,-1.52371972e+00,-7.45614711e-01,1.69765946e+00,1.37315665e+00,-7.33714966e-01,-9.80680849e-01,1.51462130e+00,2.03005222e+00,-1.01806547e-01,-8.79399954e-01,6.82643028e-02,2.21765666e+00,8.88795561e-01,-1.26676433e+00,-5.53288216e-01,1.71357194e+00,2.02640053e+00,-1.31621725e-03,-1.55033332e+00,5.33977419e-01,2.44010054e+00,5.38495574e-01,-1.10542462e+00,2.68623053e-01,2.12451877e+00,1.53319120e+00,-8.32587456e-01;0.01799363,1.64820242,0.47636838,-0.57508624,0.07439048,1.3180046,1.19962024,-0.14622492,-0.57011559,0.78212792,1.91929557,0.02884644,-1.31924115,-0.05135842,1.52154747,1.13372385,-1.21474998,-0.44887565,1.31440728,1.68346312,0.17012558,-0.76319977,1.35826373,2.12381572,0.34701723,-0.94479378,0.51769512,2.26714229,1.34180336,-0.31811004;0.84296057,2.07817528,1.74386583,-0.83389561,-1.02089465,0.81232236,1.91482288,2.01619351,-0.03027661,-0.84776254,0.06807952,1.88071141,1.33944891,-0.55516418,-0.83087822,0.73678933,2.62512139,2.03715722,-0.75741915,-1.70422096,0.48002707,1.9033749,1.67207641,-0.60761405,-0.82005395,0.48886246,2.28063097,1.69977211,-0.0362231,-1.79214949;0.85000562,1.28178314,1.25566083,-0.71461909,-0.78083153,0.80384678,2.07470049,0.52268531,-0.9477494,-0.15672233,1.08132814,1.89687869,0.54098724,-0.8210598,-0.72591957,0.95052316,1.15936462,-0.05577637,-1.05151529,0.14638354,1.51207008,1.90211368,0.19880008,-1.0150691,0.47324747,1.6892414,1.56088572,-0.54412684,-0.80208745,0.15903195;0.52351629,2.41068964,-0.05350801,-1.40157808,0.74907289,2.02066361,-0.59941591,-0.76115223,2.14050194,2.00626942,-0.77402287,-0.6237264,2.32637835,0.91411938,-1.06034131,-0.29729304,2.43348454,0.47479778,-1.07839134,0.75438917,2.76849708,-0.52135636,-1.39894308,1.60762796,1.60712128,-0.81020405,-0.68411441,2.14790089,1.92765627,-0.65487884;0\n"
     ]
    }
   ],
   "source": [
    "!head -3 data/labeled_test_mixed_sequences.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging to be level of INFO\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine CSV and label columns\n",
    "UNLABELED_CSV_COLUMNS = tag_columns\n",
    "\n",
    "LABEL_COLUMN = \"anomalous_sequence_flag\"\n",
    "LABELED_CSV_COLUMNS = UNLABELED_CSV_COLUMNS + [LABEL_COLUMN]\n",
    "\n",
    "# Set default values for each CSV column\n",
    "UNLABELED_DEFAULTS = [[\"\"] for _ in UNLABELED_CSV_COLUMNS]\n",
    "\n",
    "LABELED_DEFAULTS = UNLABELED_DEFAULTS + [[0.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(filename, mode, batch_size, params):\n",
    "  def _input_fn():\n",
    "    def decode_csv(value_column, seq_len):\n",
    "      def convert_sequences_from_strings_to_floats(features, column_list):\n",
    "        def split_and_convert_string(string_tensor):\n",
    "          # Split string tensor into a sparse tensor based on delimiter\n",
    "          split_string = tf.string_split(source = tf.expand_dims(\n",
    "            input = string_tensor, axis = 0), delimiter = \",\")\n",
    "\n",
    "          # Converts the values of the sparse tensor to floats\n",
    "          converted_tensor = tf.string_to_number(\n",
    "            string_tensor = split_string.values, \n",
    "            out_type = tf.float64)\n",
    "\n",
    "          # Create a new sparse tensor with the new converted values, \n",
    "          # because the original sparse tensor values are immutable\n",
    "          new_sparse_tensor = tf.SparseTensor(\n",
    "            indices = split_string.indices, \n",
    "            values = converted_tensor, \n",
    "            dense_shape = split_string.dense_shape)\n",
    "\n",
    "          # Create a dense tensor of the float values that were converted from text csv\n",
    "          dense_floats = tf.sparse_tensor_to_dense(\n",
    "            sp_input = new_sparse_tensor, default_value = 0.0)\n",
    "\n",
    "          dense_floats_vector = tf.squeeze(input = dense_floats, axis = 0)\n",
    "\n",
    "          return dense_floats_vector\n",
    "          \n",
    "        for column in column_list:\n",
    "          features[column] = split_and_convert_string(features[column])\n",
    "          features[column].set_shape([seq_len])\n",
    "\n",
    "        return features\n",
    "        \n",
    "      if mode == tf.estimator.ModeKeys.TRAIN or (mode == tf.estimator.ModeKeys.EVAL and params[\"evaluation_mode\"] != \"tune_anomaly_thresholds\"):\n",
    "        columns = tf.decode_csv(\n",
    "          records = value_column, \n",
    "          record_defaults = UNLABELED_DEFAULTS, \n",
    "          field_delim = \";\")\n",
    "        features = dict(zip(UNLABELED_CSV_COLUMNS, columns))\n",
    "        features = convert_sequences_from_strings_to_floats(\n",
    "          features, UNLABELED_CSV_COLUMNS)\n",
    "        return features\n",
    "      else:\n",
    "        columns = tf.decode_csv(\n",
    "          records = value_column, \n",
    "          record_defaults = LABELED_DEFAULTS, \n",
    "          field_delim = \";\")\n",
    "        features = dict(zip(LABELED_CSV_COLUMNS, columns))\n",
    "        labels = tf.cast(x = features.pop(LABEL_COLUMN), dtype = tf.float64)\n",
    "        features = convert_sequences_from_strings_to_floats(\n",
    "          features, LABELED_CSV_COLUMNS[0:-1])\n",
    "        return features, labels\n",
    "    \n",
    "    # Create list of files that match pattern\n",
    "    file_list = tf.gfile.Glob(filename = filename)\n",
    "\n",
    "    # Create dataset from file list\n",
    "    dataset = tf.data.TextLineDataset(filenames = file_list)  # Read text file\n",
    "\n",
    "    # Decode the CSV file into a features dictionary of tensors\n",
    "    dataset = dataset.map(map_func = lambda x: decode_csv(x, params[\"seq_len\"]))\n",
    "    \n",
    "    # Determine amount of times to repeat file based on if we are training or evaluating\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      num_epochs = None # indefinitely\n",
    "    else:\n",
    "      num_epochs = 1 # end-of-input after this\n",
    "\n",
    "    # Repeat files num_epoch times\n",
    "    dataset = dataset.repeat(count = num_epochs)\n",
    "\n",
    "    # Group the data into batches\n",
    "    dataset = dataset.batch(batch_size = batch_size)\n",
    "    \n",
    "    # Determine if we should shuffle based on if we are training or evaluating\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "\n",
    "    # Create a iterator and then pull the next batch of features from the example queue\n",
    "    batched_dataset = dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return batched_dataset\n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_out_input_function():\n",
    "  with tf.Session() as sess:\n",
    "    fn = read_dataset(\n",
    "      filename = \"data/labeled_validation_mixed_sequences.csv\",\n",
    "      mode = tf.estimator.ModeKeys.EVAL,\n",
    "      batch_size = 8,\n",
    "      params = {\"seq_len\": seq_len,\n",
    "          \"evaluation_mode\": \"tune_anomaly_thresholds\"})\n",
    "\n",
    "    features = sess.run(fn())\n",
    "    print(\"try_out_input_function: features = \\n{}\".format(features))\n",
    "\n",
    "#     print(\"try_out_input_function: features[tag_0].shape = {}\".format(features[\"tag_0\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_out_input_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates the count of records used\n",
    "def update_count(count_a, count_b):\n",
    "  return count_a + count_b\n",
    "\n",
    "# This function updates the mahalanobis distance variables when number_of_rows equals 1\n",
    "def singleton_batch_cov_variable_updating(\n",
    "  inner_size, \n",
    "  X, \n",
    "  count_variable, \n",
    "  mean_variable, \n",
    "  cov_variable, \n",
    "  eps):\n",
    "  # This function updates the mean vector incrementally\n",
    "  def update_mean_incremental(count_a, mean_a, value_b):\n",
    "    mean_ab = (mean_a * tf.cast(x = count_a, dtype = tf.float64) + \\\n",
    "           tf.squeeze(input = value_b, axis = 0)) / tf.cast(x = count_a + 1, dtype = tf.float64)\n",
    "    return mean_ab\n",
    "\n",
    "  # This function updates the covariance matrix incrementally\n",
    "  def update_cov_incremental(count_a, mean_a, cov_a, value_b, mean_ab, sample_cov):\n",
    "    if sample_cov == True:\n",
    "      cov_ab = (cov_a * tf.cast(x = count_a - 1, dtype = tf.float64) + \\\n",
    "            tf.matmul(a = value_b - mean_a, b = value_b - mean_ab, transpose_a = True)) \\\n",
    "        / tf.cast(x = count_a, dtype = tf.float64)\n",
    "    else:\n",
    "      cov_ab = (cov_a * tf.cast(x = count_a, dtype = tf.float64) + \\\n",
    "            tf.matmul(a = value_b - mean_a, b = value_b - mean_ab, transpose_a = True)) \\\n",
    "        / tf.cast(x = count_a + 1, dtype = tf.float64)\n",
    "    return cov_ab\n",
    "\n",
    "  # Calculate new combined mean to use for incremental covariance matrix calculation\n",
    "  mean_ab = update_mean_incremental(\n",
    "    count_a = count_variable, \n",
    "    mean_a = mean_variable, \n",
    "    value_b = X) # time_shape = (num_features,), features_shape = (sequence_length,)\n",
    "\n",
    "  # Update running variables from single example\n",
    "  count_tensor = update_count(\n",
    "    count_a = count_variable, \n",
    "    count_b = 1) # time_shape = (), features_shape = ()\n",
    "\n",
    "  mean_tensor = mean_ab # time_shape = (num_features,), features_shape = (sequence_length,)\n",
    "\n",
    "  if inner_size == 1:\n",
    "    cov_tensor = tf.zeros_like(\n",
    "      tensor = cov_variable, dtype = tf.float64)\n",
    "  else:\n",
    "    # time_shape = (num_features, num_features)\n",
    "    # features_shape = (sequence_length, sequence_length)\n",
    "    cov_tensor = update_cov_incremental(\n",
    "      count_a = count_variable, \n",
    "      mean_a = mean_variable, \n",
    "      cov_a = cov_variable, \n",
    "      value_b = X, \n",
    "      mean_ab = mean_ab, \n",
    "      sample_cov = True)\n",
    "\n",
    "  # Assign values to variables, use control dependencies around return to enforce the mahalanobis \n",
    "  # variables to be assigned, the control order matters, hence the separate contexts\n",
    "  with tf.control_dependencies(\n",
    "    control_inputs = [tf.assign(\n",
    "      ref = cov_variable, \n",
    "      value = cov_tensor)]):\n",
    "    with tf.control_dependencies(\n",
    "      control_inputs = [tf.assign(\n",
    "        ref = mean_variable, \n",
    "        value = mean_tensor)]):\n",
    "      with tf.control_dependencies(\n",
    "        control_inputs = [tf.assign(\n",
    "          ref = count_variable, \n",
    "          value = count_tensor)]):\n",
    "        return tf.identity(input = cov_variable), tf.identity(input = mean_variable), tf.identity(input = count_variable)\n",
    "\n",
    "# This function updates the mahalanobis distance variables when number_of_rows does NOT equal 1\n",
    "def non_singleton_batch_cov_variable_updating(\n",
    "  cur_batch_size, \n",
    "  inner_size, \n",
    "  X, \n",
    "  count_variable, \n",
    "  mean_variable, \n",
    "  cov_variable, \n",
    "  eps):\n",
    "  # This function updates the mean vector using a batch of data\n",
    "  def update_mean_batch(count_a, mean_a, count_b, mean_b):\n",
    "    mean_ab = (mean_a * tf.cast(x = count_a, dtype = tf.float64) + \\\n",
    "               mean_b * tf.cast(x = count_b, dtype = tf.float64)) \\\n",
    "               / tf.cast(x = count_a + count_b, dtype = tf.float64)\n",
    "    return mean_ab\n",
    "\n",
    "  # This function updates the covariance matrix using a batch of data\n",
    "  def update_cov_batch(count_a, mean_a, cov_a, count_b, mean_b, cov_b, sample_cov):\n",
    "    mean_diff = tf.expand_dims(input = mean_a - mean_b, axis = 0)\n",
    "\n",
    "    if sample_cov == True:\n",
    "      cov_ab = (cov_a * tf.cast(x = count_a - 1, dtype = tf.float64) + \\\n",
    "                cov_b * tf.cast(x = count_b - 1, dtype = tf.float64) + \\\n",
    "                tf.matmul(a = mean_diff, b = mean_diff, transpose_a = True) * \\\n",
    "                tf.cast(x = count_a * count_b, dtype = tf.float64) \\\n",
    "                / tf.cast(x = count_a + count_b, dtype = tf.float64)) \\\n",
    "                / tf.cast(x = count_a + count_b - 1, dtype = tf.float64)\n",
    "    else:\n",
    "      cov_ab = (cov_a * tf.cast(x = count_a, dtype = tf.float64) + \\\n",
    "                cov_b * tf.cast(x = count_b, dtype = tf.float64) + \\\n",
    "                tf.matmul(a = mean_diff, b = mean_diff, transpose_a = True) * \\\n",
    "                tf.cast(x = count_a * count_b, dtype = tf.float64) \\\n",
    "                / tf.cast(x = count_a + count_b, dtype = tf.float64)) \\\n",
    "                / tf.cast(x = count_a + count_b, dtype = tf.float64)\n",
    "    return cov_ab          \n",
    "\n",
    "  # Find statistics of batch\n",
    "  number_of_rows = cur_batch_size * inner_size\n",
    "\n",
    "  # time_shape = (num_features,), features_shape = (sequence_length,)\n",
    "  X_mean = tf.reduce_mean(input_tensor = X, axis = 0)\n",
    "\n",
    "  # time_shape = (cur_batch_size * sequence_length, num_features)\n",
    "  # features_shape = (cur_batch_size * num_features, sequence_length)\n",
    "  X_centered = X - X_mean\n",
    "\n",
    "  if inner_size > 1:\n",
    "    # time_shape = (num_features, num_features)\n",
    "    # features_shape = (sequence_length, sequence_length)\n",
    "    X_cov = tf.matmul(\n",
    "      a = X_centered,\n",
    "      b = X_centered, \n",
    "      transpose_a = True) / tf.cast(x = number_of_rows - 1, dtype = tf.float64)\n",
    "\n",
    "  # Update running variables from batch statistics\n",
    "  count_tensor = update_count(\n",
    "    count_a = count_variable, \n",
    "    count_b = number_of_rows) # time_shape = (), features_shape = ()\n",
    "\n",
    "  mean_tensor = update_mean_batch(\n",
    "    count_a = count_variable, \n",
    "    mean_a = mean_variable, \n",
    "    count_b = number_of_rows, \n",
    "    mean_b = X_mean) # time_shape = (num_features,), features_shape = (sequence_length,)\n",
    "\n",
    "  if inner_size == 1:\n",
    "    cov_tensor = tf.zeros_like(\n",
    "      tensor = cov_variable, dtype = tf.float64)\n",
    "  else:\n",
    "    # time_shape = (num_features, num_features)\n",
    "    # features_shape = (sequence_length, sequence_length)\n",
    "    cov_tensor = update_cov_batch(\n",
    "      count_a = count_variable, \n",
    "      mean_a = mean_variable, \n",
    "      cov_a = cov_variable, \n",
    "      count_b = number_of_rows, \n",
    "      mean_b = X_mean, \n",
    "      cov_b = X_cov, \n",
    "      sample_cov = True)\n",
    "\n",
    "  # Assign values to variables, use control dependencies around return to enforce the mahalanobis \n",
    "  # variables to be assigned, the control order matters, hence the separate contexts\n",
    "  with tf.control_dependencies(\n",
    "    control_inputs = [tf.assign(ref = cov_variable, value = cov_tensor)]):\n",
    "    with tf.control_dependencies(\n",
    "      control_inputs = [tf.assign(ref = mean_variable, value = mean_tensor)]):\n",
    "      with tf.control_dependencies(\n",
    "        control_inputs = [tf.assign(ref = count_variable, value = count_tensor)]):\n",
    "        return tf.identity(input = cov_variable), tf.identity(input = mean_variable), tf.identity(input = count_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_distance(error_vectors_reshaped, mean_vector, inv_covariance, final_shape):\n",
    "  # time_shape = (current_batch_size * seq_len, num_features)\n",
    "  # features_shape = (current_batch_size * num_features, seq_len)\n",
    "  error_vectors_reshaped_centered = error_vectors_reshaped - mean_vector\n",
    "\n",
    "  # time_shape = (num_features, current_batch_size * seq_len)\n",
    "  # features_shape = (seq_len, current_batch_size * num_features)\n",
    "  mahalanobis_right_product = tf.matmul(\n",
    "    a = inv_covariance,\n",
    "    b = error_vectors_reshaped_centered,\n",
    "    transpose_b = True)\n",
    "\n",
    "  # time_shape = (current_batch_size * seq_len, current_batch_size * seq_len)\n",
    "  # features_shape = (current_batch_size * num_features, current_batch_size * num_features)\n",
    "  mahalanobis_distance_vectorized = tf.matmul(\n",
    "    a = error_vectors_reshaped_centered,\n",
    "    b = mahalanobis_right_product)\n",
    "\n",
    "  # time_shape = (current_batch_size * seq_len,)\n",
    "  # features_shape = (current_batch_size * num_features,)\n",
    "  mahalanobis_distance_flat = tf.diag_part(input = mahalanobis_distance_vectorized)\n",
    "\n",
    "  # time_shape = (current_batch_size, seq_len)\n",
    "  # features_shape = (current_batch_size, num_features)\n",
    "  mahalanobis_distance_final_shaped = tf.reshape(\n",
    "    tensor = mahalanobis_distance_flat, \n",
    "    shape = [-1, final_shape])\n",
    "\n",
    "  # time_shape = (current_batch_size, seq_len)\n",
    "  # features_shape = (current_batch_size, num_features)\n",
    "  mahalanobis_distance_final_shaped_abs = tf.abs(x = mahalanobis_distance_final_shaped)\n",
    "\n",
    "  return mahalanobis_distance_final_shaped_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_anomaly_threshold_variables(\n",
    "  labels_normal_mask, \n",
    "  labels_anomalous_mask, \n",
    "  num_thresholds, \n",
    "  anomaly_thresholds, \n",
    "  mahalanobis_distance, \n",
    "  tp_at_thresholds_variable, \n",
    "  fn_at_thresholds_variable, \n",
    "  fp_at_thresholds_variable, \n",
    "  tn_at_thresholds_variable,\n",
    "  mode):\n",
    "  \n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    # time_shape = (num_time_anomaly_thresholds, current_batch_size, sequence_length)\n",
    "    # features_shape = (num_features_anomaly_thresholds, current_batch_size, number_of_features)\n",
    "    mahalanobis_distance_over_thresholds = tf.map_fn(\n",
    "      fn = lambda anomaly_threshold: mahalanobis_distance > anomaly_threshold, \n",
    "      elems = anomaly_thresholds, \n",
    "      dtype = tf.bool)\n",
    "  else:\n",
    "    # time_shape = (current_batch_size, sequence_length)\n",
    "    # features_shape = (current_batch_size, number_of_features)\n",
    "    mahalanobis_distance_over_thresholds = mahalanobis_distance > anomaly_thresholds\n",
    "\n",
    "  # time_shape = (num_time_anomaly_thresholds, current_batch_size)\n",
    "  # features_shape = (num_features_anomaly_thresholds, current_batch_size)    \n",
    "  mahalanobis_distance_any_over_thresholds = tf.reduce_any(\n",
    "    input_tensor = mahalanobis_distance_over_thresholds, \n",
    "    axis = -1)\n",
    "    \n",
    "  if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    # time_shape = (1, current_batch_size)\n",
    "    # features_shape = (1, current_batch_size)\n",
    "    mahalanobis_distance_any_over_thresholds = tf.expand_dims(\n",
    "      input = mahalanobis_distance_any_over_thresholds, axis = 0)\n",
    "\n",
    "  # time_shape = (num_time_anomaly_thresholds, current_batch_size)\n",
    "  # features_shape = (num_features_anomaly_thresholds, current_batch_size)\n",
    "  predicted_normals = tf.equal(\n",
    "    x = mahalanobis_distance_any_over_thresholds, \n",
    "    y = False)\n",
    "\n",
    "  # time_shape = (num_time_anomaly_thresholds, current_batch_size)\n",
    "  # features_shape = (num_features_anomaly_thresholds, current_batch_size)\n",
    "  predicted_anomalies = tf.equal(\n",
    "    x = mahalanobis_distance_any_over_thresholds, \n",
    "    y = True)\n",
    "  \n",
    "  # Calculate confusion matrix of current batch\n",
    "  # time_shape = (num_time_anomaly_thresholds,)\n",
    "  # features_shape = (num_features_anomaly_thresholds,)\n",
    "  tp = tf.reduce_sum(\n",
    "    input_tensor = tf.cast(\n",
    "      x = tf.map_fn(\n",
    "        fn = lambda threshold: tf.logical_and(\n",
    "          x = labels_anomalous_mask, \n",
    "          y = predicted_anomalies[threshold, :]), \n",
    "        elems = tf.range(start = 0, limit = num_thresholds, dtype = tf.int64), \n",
    "        dtype = tf.bool), \n",
    "      dtype = tf.int64), \n",
    "    axis = 1)\n",
    "\n",
    "  fn = tf.reduce_sum(\n",
    "    input_tensor = tf.cast(\n",
    "      x = tf.map_fn(\n",
    "        fn = lambda threshold: tf.logical_and(\n",
    "          x = labels_anomalous_mask, \n",
    "          y = predicted_normals[threshold, :]), \n",
    "        elems = tf.range(start = 0, limit = num_thresholds, dtype = tf.int64), \n",
    "        dtype = tf.bool), \n",
    "      dtype = tf.int64), \n",
    "    axis = 1)\n",
    "\n",
    "  fp = tf.reduce_sum(\n",
    "    input_tensor = tf.cast(\n",
    "      x = tf.map_fn(\n",
    "        fn = lambda threshold: tf.logical_and(\n",
    "          x = labels_normal_mask, \n",
    "          y = predicted_anomalies[threshold, :]), \n",
    "        elems = tf.range(start = 0, limit = num_thresholds, dtype = tf.int64), \n",
    "        dtype = tf.bool), \n",
    "      dtype = tf.int64), \n",
    "    axis = 1)\n",
    "\n",
    "  tn = tf.reduce_sum(\n",
    "    input_tensor = tf.cast(\n",
    "      x = tf.map_fn(\n",
    "        fn = lambda threshold: tf.logical_and(\n",
    "          x = labels_normal_mask, \n",
    "          y = predicted_normals[threshold, :]), \n",
    "        elems = tf.range(start = 0, limit = num_thresholds, dtype = tf.int64), \n",
    "        dtype = tf.bool), \n",
    "      dtype = tf.int64), \n",
    "    axis = 1)\n",
    "  \n",
    "  if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    # shape = ()\n",
    "    tp = tf.squeeze(input = tp)\n",
    "    fn = tf.squeeze(input = fn)\n",
    "    fp = tf.squeeze(input = fp)\n",
    "    tn = tf.squeeze(input = tn)\n",
    "\n",
    "  with tf.control_dependencies(\n",
    "    control_inputs = [tf.assign_add(ref = tp_at_thresholds_variable, value = tp), \n",
    "                      tf.assign_add(ref = fn_at_thresholds_variable, value = fn), \n",
    "                      tf.assign_add(ref = fp_at_thresholds_variable, value = fp), \n",
    "                      tf.assign_add(ref = tn_at_thresholds_variable, value = tn)]):\n",
    "    return tf.identity(input = tp_at_thresholds_variable), tf.identity(input = fn_at_thresholds_variable), tf.identity(input = fp_at_thresholds_variable), tf.identity(input = tn_at_thresholds_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_composite_classification_metrics(anomaly_thresholds, tp, fn, fp, tn, f_score_beta):\n",
    "  # time_shape = (num_time_anomaly_thresholds,)\n",
    "  # features_shape = (num_features_anomaly_thresholds,)\n",
    "  acc = tf.cast(x = tp + tn, dtype = tf.float64) \\\n",
    "    / tf.cast(x = tp + fn + fp + tn, dtype = tf.float64)\n",
    "  pre = tf.cast(x = tp, dtype = tf.float64) / tf.cast(x = tp + fp, dtype = tf.float64)\n",
    "  rec = tf.cast(x = tp, dtype = tf.float64) / tf.cast(x = tp + fn, dtype = tf.float64)\n",
    "  f_beta_score = (1.0 + f_score_beta ** 2) * (pre * rec) / (f_score_beta ** 2 * pre + rec)\n",
    "\n",
    "  return acc, pre, rec, f_beta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_anomaly_threshold(\n",
    "  anomaly_thresholds, f_beta_score, user_passed_anomaly_threshold, anomaly_threshold_variable):\n",
    "  if user_passed_anomaly_threshold == None:\n",
    "    best_anomaly_threshold = tf.gather(\n",
    "      params = anomaly_thresholds, \n",
    "      indices = tf.argmax(input = f_beta_score, \n",
    "      axis = 0)) # shape = ()\n",
    "  else:\n",
    "    best_anomaly_threshold = user_passed_anomaly_threshold # shape = ()\n",
    "\n",
    "  with tf.control_dependencies(\n",
    "    control_inputs = [\n",
    "      tf.assign(ref = anomaly_threshold_variable, value = best_anomaly_threshold)]):\n",
    "    return tf.identity(input = anomaly_threshold_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_autoencoder_model(X, mode, params, cur_batch_size, num_features, dummy_variable):\n",
    "  def dense_autoencoder(X, orig_dims, params):\n",
    "    def dense_encoder(X, params):\n",
    "      # Create the input layer to our DNN\n",
    "      network = X\n",
    "\n",
    "      # Add hidden layers with the given number of units/neurons per layer\n",
    "      for units in params[\"encoder_dnn_hidden_units\"]:\n",
    "        network = tf.layers.dense(\n",
    "          inputs = network, \n",
    "          units = units, \n",
    "          activation = tf.nn.relu)\n",
    "\n",
    "      latent_vector = tf.layers.dense(\n",
    "        inputs = network, \n",
    "        units = params[\"latent_vector_size\"], \n",
    "        activation = tf.nn.relu)\n",
    "\n",
    "      return latent_vector\n",
    "\n",
    "    def dense_decoder(latent_vector, orig_dims, params):\n",
    "      # Create the input layer to our DNN\n",
    "      network = latent_vector\n",
    "\n",
    "      # Add hidden layers with the given number of units/neurons per layer\n",
    "      for units in params[\"decoder_dnn_hidden_units\"][::-1]:\n",
    "        network = tf.layers.dense(\n",
    "          inputs = network, \n",
    "          units = units, \n",
    "          activation = tf.nn.relu)\n",
    "\n",
    "      output_vector = tf.layers.dense(\n",
    "        inputs = network, \n",
    "        units = orig_dims, \n",
    "        activation = tf.nn.relu)\n",
    "    \n",
    "      return output_vector\n",
    "    \n",
    "    latent_vector = dense_encoder(X, params)\n",
    "    output_vector = dense_decoder(latent_vector, orig_dims, params)\n",
    "    \n",
    "    return output_vector\n",
    "  \n",
    "  # Reshape into 2-D tensors\n",
    "  # Time based\n",
    "  # shape = (cur_batch_size * seq_len, num_features)\n",
    "  X_time = tf.reshape(\n",
    "    tensor = X, \n",
    "    shape = [cur_batch_size * params[\"seq_len\"], num_features])\n",
    "  \n",
    "  # shape = (cur_batch_size * seq_len, num_features)\n",
    "  X_time_recon = dense_autoencoder(X_time, num_features, params)\n",
    "  \n",
    "  # Features based\n",
    "  # shape = (cur_batch_size, num_features, seq_len)\n",
    "  X_transposed = tf.transpose(a = X, perm = [0, 2, 1])\n",
    "  # shape = (cur_batch_size * num_features, seq_len)\n",
    "  X_features = tf.reshape(\n",
    "    tensor = X_transposed, \n",
    "    shape = [cur_batch_size * num_features, params[\"seq_len\"]])\n",
    "  \n",
    "  # shape = (cur_batch_size * num_features, seq_len)\n",
    "  X_features_recon = dense_autoencoder(X_features, params[\"seq_len\"], params)\n",
    "  \n",
    "  if mode == tf.estimator.ModeKeys.TRAIN and params[\"evaluation_mode\"] == \"reconstruction\":\n",
    "    X_time_recon_3d = tf.reshape(\n",
    "      tensor = X_time_recon, \n",
    "      shape = [cur_batch_size, params[\"seq_len\"], num_features])\n",
    "    X_features_recon_3d = tf.transpose(\n",
    "      a = tf.reshape(\n",
    "        tensor = X_features_recon, \n",
    "        shape = [cur_batch_size, num_features, params[\"seq_len\"]]), \n",
    "      perm = [0, 2, 1])\n",
    "    \n",
    "    X_time_recon_3d_weighted = X_time_recon_3d * params[\"time_loss_weight\"]\n",
    "    X_features_recon_3d_weighted = X_features_recon_3d * params[\"features_loss_weight\"]\n",
    "    \n",
    "    predictions = (X_time_recon_3d_weighted + X_features_recon_3d_weighted) \\\n",
    "      / (params[\"time_loss_weight\"] + params[\"features_loss_weight\"])\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(labels = X, predictions = predictions)\n",
    "\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss = loss,\n",
    "      global_step = tf.train.get_global_step(),\n",
    "      learning_rate = params[\"learning_rate\"],\n",
    "      optimizer = \"Adam\")\n",
    "    \n",
    "    return loss, train_op, None, None, None, None\n",
    "  else:\n",
    "    return None, None, X_time, X_time_recon, X_features, X_features_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_encoder_decoder_autoencoder_model(X, mode, params, cur_batch_size, num_features, dummy_variable):\n",
    "  def create_LSTM_stack(lstm_hidden_units, lstm_dropout_output_keep_probs):\n",
    "    # First create a list of LSTM cells using our list of lstm hidden unit sizes\n",
    "    lstm_cells = [tf.contrib.rnn.BasicLSTMCell(\n",
    "      num_units = units, \n",
    "      forget_bias = 1.0, \n",
    "      state_is_tuple = True) for units in lstm_hidden_units] # list of LSTM cells\n",
    "\n",
    "    # Next apply a dropout wrapper to our stack of LSTM cells, in this case just on the outputs\n",
    "    dropout_lstm_cells = [tf.nn.rnn_cell.DropoutWrapper(\n",
    "      cell = lstm_cells[cell_index], \n",
    "      input_keep_prob = 1.0, \n",
    "      output_keep_prob = lstm_dropout_output_keep_probs[cell_index], \n",
    "      state_keep_prob = 1.0) for cell_index in range(len(lstm_cells))]\n",
    "\n",
    "    # Create a stack of layers of LSTM cells\n",
    "    stacked_lstm_cells = tf.contrib.rnn.MultiRNNCell(\n",
    "      cells = dropout_lstm_cells, \n",
    "      state_is_tuple = True) # combines list into MultiRNNCell object\n",
    "\n",
    "    return stacked_lstm_cells\n",
    "\n",
    "  # The rnn_decoder function takes labels during TRAIN/EVAL \n",
    "  # and a start token followed by its previous predictions during PREDICT\n",
    "  # Starts with an intial state of the final encoder states\n",
    "  def rnn_decoder(decoder_inputs, initial_state, cell, inference, dnn_hidden_units, num_features):\n",
    "    # Create the decoder variable scope\n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "      # Load in our initial state from our encoder\n",
    "      state = initial_state # tuple of final encoder c_state and h_state of final encoder layer\n",
    "\n",
    "      # Create an empty list to store our hidden state output for every timestep\n",
    "      outputs = []\n",
    "\n",
    "      # Begin with no previous output\n",
    "      previous_output = None\n",
    "\n",
    "      # Loop over all of our decoder_inputs which will be seq_len long\n",
    "      for index, decoder_input in enumerate(decoder_inputs):\n",
    "        # If there has been a previous output then we will determine the next input\n",
    "        if previous_output is not None:\n",
    "          # Create the input layer to our DNN\n",
    "          network = previous_output # shape = (cur_batch_size, lstm_hidden_units[-1])\n",
    "\n",
    "          # Create our dnn variable scope\n",
    "          with tf.variable_scope(name_or_scope = \"dnn\", reuse = tf.AUTO_REUSE):\n",
    "            # Add hidden layers with the given number of units/neurons per layer\n",
    "            # shape = (cur_batch_size, dnn_hidden_units[i])\n",
    "            for units in dnn_hidden_units:\n",
    "              network = tf.layers.dense(\n",
    "                inputs = network, \n",
    "                units = units, \n",
    "                activation = tf.nn.relu)\n",
    "\n",
    "            # Connect final hidden layer to linear layer to get the logits\n",
    "            logits = tf.layers.dense(\n",
    "              inputs = network, \n",
    "              units = num_features, \n",
    "              activation = None) # shape = (cur_batch_size, num_features)\n",
    "\n",
    "          # If we are in inference then we will overwrite our next decoder_input \n",
    "          # with the logits we just calculated.\n",
    "          # Otherwise, we leave the decoder_input input as it was from the enumerated list\n",
    "          # We have to calculate the logits even when not using them so that the correct \n",
    "          # dnn subgraph will be generated here and after the encoder-decoder for both \n",
    "          # training and inference\n",
    "          if inference == True:\n",
    "            decoder_input = logits # shape = (cur_batch_size, num_features)\n",
    "\n",
    "        # If this isn\"t our first time through the loop, just reuse(share) the same \n",
    "        # variables for each iteration within the current variable scope\n",
    "        if index > 0:\n",
    "          tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "        # Run the decoder input through the decoder stack picking up from the previous state\n",
    "        # output_shape = (cur_batch_size, lstm_hidden_units[-1])\n",
    "        # state_shape = # tuple of final decoder c_state and h_state\n",
    "        output, state = cell(decoder_input, state)\n",
    "\n",
    "        # Append the current decoder hidden state output to the outputs list\n",
    "        # list eventually seq_len long of shape = (cur_batch_size, lstm_hidden_units[-1])\n",
    "        outputs.append(output)\n",
    "\n",
    "        # Set the previous output to the output just calculated\n",
    "        previous_output = output # shape = (cur_batch_size, lstm_hidden_units[-1])\n",
    "    return outputs, state\n",
    "  \n",
    "  # Unstack all of 3-D features tensor into a sequence(list) of 2-D tensors of \n",
    "  # shape = (cur_batch_size, num_features)\n",
    "  X_sequence = tf.unstack(value = X, num = params[\"seq_len\"], axis = 1)\n",
    "\n",
    "  # Since this is an autoencoder, the features are the labels. \n",
    "  # It often works better though to have the labels in reverse order\n",
    "  if params[\"reverse_labels_sequence\"] == True:\n",
    "    Y = tf.reverse_sequence(\n",
    "      input = X,  # shape = (cur_batch_size, seq_len, num_features)\n",
    "      seq_lengths = tf.tile(\n",
    "        input = tf.constant(value = [params[\"seq_len\"]], dtype = tf.int64), \n",
    "        multiples = tf.expand_dims(input = cur_batch_size, axis = 0)), \n",
    "      seq_axis = 1, \n",
    "      batch_axis = 0)\n",
    "  else:\n",
    "    Y = X  # shape = (cur_batch_size, seq_len, num_features)\n",
    "  \n",
    "  ################################################################################\n",
    "  \n",
    "  # Create encoder of encoder-decoder LSTM stacks\n",
    "  \n",
    "  # Create our decoder now\n",
    "  decoder_stacked_lstm_cells = create_LSTM_stack(\n",
    "    params[\"decoder_lstm_hidden_units\"], \n",
    "    params[\"lstm_dropout_output_keep_probs\"])\n",
    "  \n",
    "  # Create the encoder variable scope\n",
    "  with tf.variable_scope(\"encoder\"):\n",
    "    # Create separate encoder cells with their own weights separate from decoder\n",
    "    encoder_stacked_lstm_cells = create_LSTM_stack(\n",
    "      params[\"encoder_lstm_hidden_units\"], \n",
    "      params[\"lstm_dropout_output_keep_probs\"])\n",
    "\n",
    "    # Encode the input sequence using our encoder stack of LSTMs\n",
    "    # encoder_outputs = seq_len long of shape = (cur_batch_size, encoder_lstm_hidden_units[-1])\n",
    "    # encoder_states = tuple of final encoder c_state and h_state for each layer\n",
    "    encoder_outputs, encoder_states = tf.nn.static_rnn(\n",
    "      cell = encoder_stacked_lstm_cells, \n",
    "      inputs = X_sequence, \n",
    "      initial_state = encoder_stacked_lstm_cells.zero_state(\n",
    "        batch_size = tf.cast(x = cur_batch_size, dtype = tf.int32), \n",
    "        dtype = tf.float64), \n",
    "      dtype = tf.float64)\n",
    "\n",
    "    # We just pass on the final c and h states of the encoder\"s last layer, \n",
    "    # so extract that and drop the others\n",
    "    # LSTMStateTuple shape = (cur_batch_size, lstm_hidden_units[-1])\n",
    "    encoder_final_states = encoder_states[-1]\n",
    "\n",
    "    # Extract the c and h states from the tuple\n",
    "    # both have shape = (cur_batch_size, lstm_hidden_units[-1])\n",
    "    encoder_final_c, encoder_final_h = encoder_final_states\n",
    "\n",
    "    # In case the decoder\"s first layer\"s number of units is different than encoder's last \n",
    "    # layer's number of units, use a dense layer to map to the correct shape\n",
    "    encoder_final_c_dense = tf.layers.dense(\n",
    "      inputs = encoder_final_c, \n",
    "      units = params[\"decoder_lstm_hidden_units\"][0], \n",
    "      activation = None) # shape = (cur_batch_size, decoder_lstm_hidden_units[0])\n",
    "    encoder_final_h_dense = tf.layers.dense(\n",
    "      inputs = encoder_final_h, \n",
    "      units = params[\"decoder_lstm_hidden_units\"][0], \n",
    "      activation = None) # shape = (cur_batch_size, decoder_lstm_hidden_units[0])\n",
    "\n",
    "    # The decoder\"s first layer\"s state comes from the encoder, \n",
    "    # the rest of the layers\" initial states are zero\n",
    "    decoder_intial_states = tuple(\n",
    "      [tf.contrib.rnn.LSTMStateTuple(c = encoder_final_c_dense, h = encoder_final_h_dense)] + \\\n",
    "      [tf.contrib.rnn.LSTMStateTuple(\n",
    "        c = tf.zeros(shape = [cur_batch_size, units], dtype = tf.float64), \n",
    "        h = tf.zeros(shape = [cur_batch_size, units], dtype = tf.float64)) \n",
    "      for units in params[\"decoder_lstm_hidden_units\"][1:]])\n",
    "  \n",
    "  ################################################################################\n",
    "\n",
    "  # Create decoder of encoder-decoder LSTM stacks\n",
    "  \n",
    "  # Train our decoder now\n",
    "  \n",
    "  # Encoder-decoders work differently during training/evaluation and inference \n",
    "  # so we will have two separate subgraphs for each\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN and params[\"evaluation_mode\"] == \"reconstruction\":\n",
    "    # Break 3-D labels tensor into a list of 2-D tensors of shape = (cur_batch_size, num_features)\n",
    "    unstacked_labels = tf.unstack(value = Y, num = params[\"seq_len\"], axis = 1)\n",
    "\n",
    "    # Call our decoder using the labels as our inputs, the encoder final state as our \n",
    "    # initial state, our other LSTM stack as our cells, and inference set to false\n",
    "    decoder_outputs, decoder_states = rnn_decoder(\n",
    "      decoder_inputs = unstacked_labels, \n",
    "      initial_state = decoder_intial_states, \n",
    "      cell = decoder_stacked_lstm_cells, \n",
    "      inference = False,\n",
    "      dnn_hidden_units = params[\"dnn_hidden_units\"],\n",
    "      num_features = num_features)\n",
    "  else:\n",
    "    # Since this is inference create fake labels. The list length needs to be the output \n",
    "    # sequence length even though only the first element is the only one actually used \n",
    "    # (as our go signal)\n",
    "    fake_labels = [tf.zeros(shape = [cur_batch_size, num_features], dtype = tf.float64) \n",
    "      for _ in range(params[\"seq_len\"])]\n",
    "    \n",
    "    # Call our decoder using fake labels as our inputs, the encoder final state as our initial \n",
    "    # state, our other LSTM stack as our cells, and inference set to true\n",
    "    # decoder_outputs = seq_len long of shape = (cur_batch_size, decoder_lstm_hidden_units[-1])\n",
    "    # decoder_states = tuple of final decoder c_state and h_state for each layer\n",
    "    decoder_outputs, decoder_states = rnn_decoder(\n",
    "      decoder_inputs = fake_labels, \n",
    "      initial_state = decoder_intial_states, \n",
    "      cell = decoder_stacked_lstm_cells, \n",
    "      inference = True,\n",
    "      dnn_hidden_units = params[\"dnn_hidden_units\"],\n",
    "      num_features = num_features)\n",
    "  \n",
    "  # Stack together the list of rank 2 decoder output tensors into one rank 3 tensor of\n",
    "  # shape = (cur_batch_size, seq_len, lstm_hidden_units[-1])\n",
    "  stacked_decoder_outputs = tf.stack(values = decoder_outputs, axis = 1)\n",
    "  \n",
    "  # Reshape rank 3 decoder outputs into rank 2 by folding sequence length into batch size\n",
    "  # shape = (cur_batch_size * seq_len, lstm_hidden_units[-1])\n",
    "  reshaped_stacked_decoder_outputs = tf.reshape(\n",
    "    tensor = stacked_decoder_outputs, \n",
    "    shape = [cur_batch_size * params[\"seq_len\"], params[\"decoder_lstm_hidden_units\"][-1]])\n",
    "\n",
    "  ################################################################################\n",
    "  \n",
    "  # Create the DNN structure now after the encoder-decoder LSTM stack\n",
    "  # Create the input layer to our DNN\n",
    "  # shape = (cur_batch_size * seq_len, lstm_hidden_units[-1])\n",
    "  network = reshaped_stacked_decoder_outputs\n",
    "  \n",
    "  # Reuse the same variable scope as we used within our decoder (for inference)\n",
    "  with tf.variable_scope(name_or_scope = \"dnn\", reuse = tf.AUTO_REUSE):\n",
    "    # Add hidden layers with the given number of units/neurons per layer\n",
    "    for units in params[\"dnn_hidden_units\"]:\n",
    "      network = tf.layers.dense(\n",
    "        inputs = network, \n",
    "        units = units, \n",
    "        activation = tf.nn.relu) # shape = (cur_batch_size * seq_len, dnn_hidden_units[i])\n",
    "\n",
    "    # Connect the final hidden layer to a dense layer with no activation to get the logits\n",
    "    logits = tf.layers.dense(\n",
    "      inputs = network, \n",
    "      units = num_features, \n",
    "      activation = None) # shape = (cur_batch_size * seq_len, num_features)\n",
    "  \n",
    "  # Now that we are through the final DNN for each sequence element for each example in the batch,\n",
    "  # reshape the predictions to match our labels.\n",
    "  # shape = (cur_batch_size, seq_len, num_features)\n",
    "  predictions = tf.reshape(\n",
    "    tensor = logits, \n",
    "    shape = [cur_batch_size, params[\"seq_len\"], num_features])\n",
    "  \n",
    "  if mode == tf.estimator.ModeKeys.TRAIN and params[\"evaluation_mode\"] == \"reconstruction\":\n",
    "    loss = tf.losses.mean_squared_error(labels = Y, predictions = predictions)\n",
    "\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss = loss,\n",
    "      global_step = tf.train.get_global_step(),\n",
    "      learning_rate = params[\"learning_rate\"],\n",
    "      optimizer = \"Adam\")\n",
    "    \n",
    "    return loss, train_op, None, None, None, None\n",
    "  else:\n",
    "    if params[\"reverse_labels_sequence\"] == True:\n",
    "      predictions = tf.reverse_sequence(\n",
    "        input = predictions,  # shape = (cur_batch_size, seq_len, num_features)\n",
    "        seq_lengths = tf.tile(\n",
    "          input = tf.constant(value = [params[\"seq_len\"]], dtype = tf.int64), \n",
    "          multiples = tf.expand_dims(input = cur_batch_size, axis = 0)), \n",
    "        seq_axis = 1, \n",
    "        batch_axis = 0)\n",
    "    \n",
    "    # Reshape into 2-D tensors\n",
    "    # Time based\n",
    "    # shape = (cur_batch_size * seq_len, num_features)\n",
    "    X_time = tf.reshape(\n",
    "      tensor = X, \n",
    "      shape = [cur_batch_size * params[\"seq_len\"], num_features])\n",
    "    \n",
    "    X_time_reconstructed = tf.reshape(\n",
    "      tensor = predictions, \n",
    "      shape = [cur_batch_size * params[\"seq_len\"], num_features])\n",
    "\n",
    "    # Features based\n",
    "    # shape = (cur_batch_size, num_features, seq_len)\n",
    "    X_transposed = tf.transpose(a = X, perm = [0, 2, 1])\n",
    "    # shape = (cur_batch_size * num_features, seq_len)\n",
    "    X_features = tf.reshape(\n",
    "      tensor = X_transposed, \n",
    "      shape = [cur_batch_size * num_features, params[\"seq_len\"]])\n",
    "    \n",
    "    # shape = (cur_batch_size, num_features, seq_len)\n",
    "    predictions_transposed = tf.transpose(a = predictions, perm = [0, 2, 1])\n",
    "    # shape = (cur_batch_size * num_features, seq_len)\n",
    "    X_features_reconstructed = tf.reshape(\n",
    "      tensor = predictions_transposed, \n",
    "      shape = [cur_batch_size * num_features, params[\"seq_len\"]])\n",
    "    \n",
    "    return None, None, X_time, X_time_reconstructed, X_features, X_features_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_model(X, mode, params, cur_batch_size, num_features, dummy_variable):\n",
    "  # Reshape into 2-D tensors\n",
    "  # Time based\n",
    "  # shape = (cur_batch_size * seq_len, num_features)\n",
    "  X_time = tf.reshape(\n",
    "    tensor = X, \n",
    "    shape = [cur_batch_size * params[\"seq_len\"], num_features])\n",
    "  \n",
    "  # Features based\n",
    "  # shape = (cur_batch_size, num_features, seq_len)\n",
    "  X_transposed = tf.transpose(a = X, perm = [0, 2, 1])\n",
    "  # shape = (cur_batch_size * num_features, seq_len)\n",
    "  X_features = tf.reshape(\n",
    "    tensor = X_transposed, \n",
    "    shape = [cur_batch_size * num_features, params[\"seq_len\"]])\n",
    "  \n",
    "  ################################################################################\n",
    "  \n",
    "  # Variables for calculating error distribution statistics\n",
    "  with tf.variable_scope(name_or_scope = \"pca_variables\", reuse = tf.AUTO_REUSE):\n",
    "    # Time based\n",
    "    pca_time_count_variable = tf.get_variable(\n",
    "      name = \"pca_time_count_variable\", # shape = ()\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_time_mean_variable = tf.get_variable(\n",
    "      name = \"pca_time_mean_variable\", # shape = (num_features,)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [num_features],  dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_time_cov_variable = tf.get_variable(\n",
    "      name = \"pca_time_cov_variable\", # shape = (num_features, num_features)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [num_features, num_features], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_time_eigenvalues_variable = tf.get_variable(\n",
    "      name = \"pca_time_eigenvalues_variable\", # shape = (num_features,)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [num_features], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_time_eigenvectors_variable = tf.get_variable(\n",
    "      name = \"pca_time_eigenvectors_variable\", # shape = (num_features, num_features)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [num_features, num_features], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "    # Features based\n",
    "    pca_features_count_variable = tf.get_variable(\n",
    "      name = \"pca_features_count_variable\", # shape = ()\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_features_mean_variable = tf.get_variable(\n",
    "      name = \"pca_features_mean_variable\", # shape = (seq_len,)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [params[\"seq_len\"]], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_features_cov_variable = tf.get_variable(\n",
    "      name = \"pca_features_cov_variable\", # shape = (seq_len, seq_len)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [params[\"seq_len\"], params[\"seq_len\"]], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_features_eigenvalues_variable = tf.get_variable(\n",
    "      name = \"pca_features_eigenvalues_variable\", # shape = (seq_len,)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [params[\"seq_len\"]], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_features_eigenvectors_variable = tf.get_variable(\n",
    "      name = \"pca_features_eigenvectors_variable\", # shape = (seq_len, seq_len)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [params[\"seq_len\"], params[\"seq_len\"]], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "    \n",
    "  # 3. Loss function, training/eval ops\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN and params[\"evaluation_mode\"] == \"reconstruction\":\n",
    "    with tf.variable_scope(name_or_scope = \"pca_variables\", reuse = tf.AUTO_REUSE):\n",
    "      # Check if batch is a singleton or not, very important for covariance math\n",
    "\n",
    "      # Time based ########################################\n",
    "      singleton_condition = tf.equal(\n",
    "        x = cur_batch_size * params[\"seq_len\"], y = 1) # shape = ()\n",
    "\n",
    "      pca_time_cov_variable, pca_time_mean_variable, pca_time_count_variable = tf.cond(\n",
    "        pred = singleton_condition, \n",
    "        true_fn = lambda: singleton_batch_cov_variable_updating(\n",
    "          params[\"seq_len\"], \n",
    "          X_time, \n",
    "          pca_time_count_variable, \n",
    "          pca_time_mean_variable, \n",
    "          pca_time_cov_variable,\n",
    "          params[\"eps\"]), \n",
    "        false_fn = lambda: non_singleton_batch_cov_variable_updating(\n",
    "          cur_batch_size, \n",
    "          params[\"seq_len\"], \n",
    "          X_time, \n",
    "          pca_time_count_variable, \n",
    "          pca_time_mean_variable, \n",
    "          pca_time_cov_variable,\n",
    "          params[\"eps\"]))\n",
    "\n",
    "      pca_time_eigenvalues_tensor, pca_time_eigenvectors_tensor = tf.linalg.eigh(\n",
    "        tensor = pca_time_cov_variable) # shape = (num_features,) & (num_features, num_features)\n",
    "\n",
    "      # Features based ########################################\n",
    "      singleton_features_condition = tf.equal(\n",
    "        x = cur_batch_size * num_features, y = 1) # shape = ()\n",
    "\n",
    "      pca_features_cov_variable, pca_features_mean_variable, pca_features_count_variable = tf.cond(\n",
    "        pred = singleton_features_condition, \n",
    "        true_fn = lambda: singleton_batch_cov_variable_updating(\n",
    "          num_features, \n",
    "          X_features, \n",
    "          pca_features_count_variable, pca_features_mean_variable, \n",
    "          pca_features_cov_variable,\n",
    "          params[\"eps\"]), \n",
    "        false_fn = lambda: non_singleton_batch_cov_variable_updating(\n",
    "          cur_batch_size, \n",
    "          num_features, \n",
    "          X_features, \n",
    "          pca_features_count_variable, \n",
    "          pca_features_mean_variable, \n",
    "          pca_features_cov_variable,\n",
    "          params[\"eps\"]))\n",
    "\n",
    "      pca_features_eigenvalues_tensor, pca_features_eigenvectors_tensor = tf.linalg.eigh(\n",
    "        tensor = pca_features_cov_variable) # shape = (seq_len,) & (seq_len, seq_len)\n",
    "\n",
    "    # Lastly use control dependencies around loss to enforce the mahalanobis variables to be assigned, the control order matters, hence the separate contexts\n",
    "    with tf.control_dependencies(\n",
    "      control_inputs = [pca_time_cov_variable, pca_features_cov_variable]):\n",
    "      with tf.control_dependencies(\n",
    "        control_inputs = [pca_time_mean_variable, pca_features_mean_variable]):\n",
    "        with tf.control_dependencies(\n",
    "          control_inputs = [pca_time_count_variable, pca_features_count_variable]):\n",
    "          with tf.control_dependencies(\n",
    "            control_inputs = [tf.assign(ref = pca_time_eigenvalues_variable, value = pca_time_eigenvalues_tensor), \n",
    "                              tf.assign(ref = pca_time_eigenvectors_variable, value = pca_time_eigenvectors_tensor),\n",
    "                              tf.assign(ref = pca_features_eigenvalues_variable, value = pca_features_eigenvalues_tensor), \n",
    "                              tf.assign(ref = pca_features_eigenvectors_variable, value = pca_features_eigenvectors_tensor)]):\n",
    "            loss = tf.reduce_sum(input_tensor = tf.zeros(shape = (), dtype = tf.float64) * dummy_variable)\n",
    "\n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "              loss = loss,\n",
    "              global_step = tf.train.get_global_step(),\n",
    "              learning_rate = params[\"learning_rate\"],\n",
    "              optimizer = \"SGD\")\n",
    "            \n",
    "            return loss, train_op, None, None, None, None\n",
    "  else:\n",
    "    # Time based\n",
    "    # shape = (cur_batch_size * seq_len, num_features)\n",
    "    X_time_centered = X_time - pca_time_mean_variable\n",
    "    # shape = (cur_batch_size * seq_len, params[\"k_principal_components\"])\n",
    "    X_time_projected = tf.matmul(\n",
    "      a = X_time_centered, \n",
    "      b = pca_time_eigenvectors_variable[:, -params[\"k_principal_components\"]:])\n",
    "    # shape = (cur_batch_size * seq_len, num_features)\n",
    "    X_time_reconstructed = tf.matmul(\n",
    "      a = X_time_projected, \n",
    "      b = pca_time_eigenvectors_variable[:, -params[\"k_principal_components\"]:], \n",
    "      transpose_b = True)\n",
    "\n",
    "    # Features based\n",
    "    # shape = (cur_batch_size * num_features, seq_len)\n",
    "    X_features_centered = X_features - pca_features_mean_variable\n",
    "    # shape = (cur_batch_size * num_features, params[\"k_principal_components\"])\n",
    "    X_features_projected = tf.matmul(\n",
    "      a = X_features_centered, \n",
    "      b = pca_features_eigenvectors_variable[:, -params[\"k_principal_components\"]:])\n",
    "    # shape = (cur_batch_size * num_features, seq_len)\n",
    "    X_features_reconstructed = tf.matmul(\n",
    "      a = X_features_projected, \n",
    "      b = pca_features_eigenvectors_variable[:, -params[\"k_principal_components\"]:], \n",
    "      transpose_b = True)\n",
    "    \n",
    "    return None, None, X_time_centered, X_time_reconstructed, X_features_centered, X_features_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our model function to be used in our custom estimator\n",
    "def anomaly_detection(features, labels, mode, params):\n",
    "  print(\"\\nanomaly_detection: features = \\n{}\".format(features))\n",
    "  print(\"anomaly_detection: labels = \\n{}\".format(labels))\n",
    "  print(\"anomaly_detection: mode = \\n{}\".format(mode))\n",
    "  print(\"anomaly_detection: params = \\n{}\".format(params))\n",
    "\n",
    "  # 0. Get input sequence tensor into correct shape\n",
    "  # Get dynamic batch size in case there was a partially filled batch\n",
    "  cur_batch_size = tf.shape(\n",
    "    input = features[UNLABELED_CSV_COLUMNS[0]], out_type = tf.int64)[0]\n",
    "\n",
    "  # Get the number of features \n",
    "  num_features = len(UNLABELED_CSV_COLUMNS)\n",
    "\n",
    "  # Stack all of the features into a 3-D tensor\n",
    "  X = tf.stack(\n",
    "    values = [features[key] for key in UNLABELED_CSV_COLUMNS], \n",
    "    axis = 2) # shape = (cur_batch_size, seq_len, num_features)\n",
    "  \n",
    "  ################################################################################\n",
    "\n",
    "  # Variables for calculating error distribution statistics\n",
    "  with tf.variable_scope(\n",
    "    name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "    # Time based\n",
    "    abs_err_count_time_variable = tf.get_variable(\n",
    "      name = \"abs_err_count_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    abs_err_mean_time_variable = tf.get_variable(\n",
    "      name = \"abs_err_mean_time_variable\",\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [num_features], dtype = tf.float64),\n",
    "      trainable = False) # shape = (num_features,)\n",
    "\n",
    "    abs_err_cov_time_variable = tf.get_variable(\n",
    "      name = \"abs_err_cov_time_variable\",\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [num_features, num_features], dtype = tf.float64),\n",
    "      trainable = False) # shape = (num_features, num_features)\n",
    "\n",
    "    abs_err_inv_cov_time_variable = tf.get_variable(\n",
    "      name = \"abs_err_inv_cov_time_variable\",\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [num_features, num_features], dtype = tf.float64),\n",
    "      trainable = False) # shape = (num_features, num_features)\n",
    "\n",
    "    # Features based\n",
    "    abs_err_count_features_variable = tf.get_variable(\n",
    "      name = \"abs_err_count_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    abs_err_mean_features_variable = tf.get_variable(\n",
    "      name = \"abs_err_mean_features_variable\",\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [params[\"seq_len\"]], dtype = tf.float64),\n",
    "      trainable = False) # shape = (seq_len,)\n",
    "\n",
    "    abs_err_cov_features_variable = tf.get_variable(\n",
    "      name = \"abs_err_cov_features_variable\",\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [params[\"seq_len\"], params[\"seq_len\"]], dtype = tf.float64),\n",
    "      trainable = False) # shape = (seq_len, seq_len)\n",
    "\n",
    "    abs_err_inv_cov_features_variable = tf.get_variable(\n",
    "      name = \"abs_err_inv_cov_features_variable\",\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [params[\"seq_len\"], params[\"seq_len\"]], dtype = tf.float64),\n",
    "      trainable = False) # shape = (seq_len, seq_len)\n",
    "  \n",
    "  # Variables for automatically tuning anomaly thresholds\n",
    "  with tf.variable_scope(\n",
    "    name_or_scope = \"mahalanobis_distance_threshold_variables\", reuse = tf.AUTO_REUSE):\n",
    "    # Time based\n",
    "    tp_at_thresholds_time_variable = tf.get_variable(\n",
    "      name = \"tp_at_thresholds_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_time_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_time_anomaly_thresholds,)\n",
    "\n",
    "    fn_at_thresholds_time_variable = tf.get_variable(\n",
    "      name = \"fn_at_thresholds_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_time_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_time_anomaly_thresholds,)\n",
    "\n",
    "    fp_at_thresholds_time_variable = tf.get_variable(\n",
    "      name = \"fp_at_thresholds_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_time_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_time_anomaly_thresholds,)\n",
    "\n",
    "    tn_at_thresholds_time_variable = tf.get_variable(\n",
    "      name = \"tn_at_thresholds_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_time_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_time_anomaly_thresholds,)\n",
    "\n",
    "    time_anomaly_threshold_variable = tf.get_variable(\n",
    "      name = \"time_anomaly_threshold_variable\",\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.float64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    # Features based\n",
    "    tp_at_thresholds_features_variable = tf.get_variable(\n",
    "      name = \"tp_at_thresholds_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_features_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_features_anomaly_thresholds,)\n",
    "\n",
    "    fn_at_thresholds_features_variable = tf.get_variable(\n",
    "      name = \"fn_at_thresholds_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_features_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_features_anomaly_thresholds,)\n",
    "\n",
    "    fp_at_thresholds_features_variable = tf.get_variable(\n",
    "      name = \"fp_at_thresholds_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_features_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_features_anomaly_thresholds,)\n",
    "\n",
    "    tn_at_thresholds_features_variable = tf.get_variable(\n",
    "      name = \"tn_at_thresholds_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_features_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_features_anomaly_thresholds,)\n",
    "\n",
    "    features_anomaly_threshold_variable = tf.get_variable(\n",
    "      name = \"features_anomaly_threshold_variable\", # shape = ()\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "  # Variables for automatically tuning anomaly thresholds\n",
    "  with tf.variable_scope(\n",
    "    name_or_scope = \"anomaly_threshold_eval_variables\", reuse = tf.AUTO_REUSE):\n",
    "    # Time based\n",
    "    tp_at_threshold_eval_time_variable = tf.get_variable(\n",
    "      name = \"tp_at_threshold_eval_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    fn_at_threshold_eval_time_variable = tf.get_variable(\n",
    "      name = \"fn_at_threshold_eval_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    fp_at_threshold_eval_time_variable = tf.get_variable(\n",
    "      name = \"fp_at_threshold_eval_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    tn_at_threshold_eval_time_variable = tf.get_variable(\n",
    "      name = \"tn_at_threshold_eval_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    # Features based\n",
    "    tp_at_threshold_eval_features_variable = tf.get_variable(\n",
    "      name = \"tp_at_threshold_eval_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    fn_at_threshold_eval_features_variable = tf.get_variable(\n",
    "      name = \"fn_at_threshold_eval_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    fp_at_threshold_eval_features_variable = tf.get_variable(\n",
    "      name = \"fp_at_threshold_eval_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    tn_at_threshold_eval_features_variable = tf.get_variable(\n",
    "      name = \"tn_at_threshold_eval_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "  \n",
    "  dummy_variable = tf.get_variable(\n",
    "    name = \"dummy_variable\",\n",
    "    dtype = tf.float64,\n",
    "    initializer = tf.zeros(shape = [], dtype = tf.float64),\n",
    "    trainable = True) # shape = ()\n",
    "  \n",
    "################################################################################\n",
    "  \n",
    "  predictions_dict = None\n",
    "  loss = None\n",
    "  train_op = None\n",
    "  eval_metric_ops = None\n",
    "  export_outputs = None\n",
    "  \n",
    "  # Now branch off based on which mode we are in\n",
    "  \n",
    "  # Call specific model\n",
    "  model_functions = {\n",
    "    \"dense_autoencoder\": dense_autoencoder_model,\n",
    "    \"lstm_encoder_decoder_autoencoder\": lstm_encoder_decoder_autoencoder_model,\n",
    "    \"pca\": pca_model}\n",
    "\n",
    "  # Get function pointer for selected model type\n",
    "  model_function = model_functions[params[\"model_type\"]]\n",
    "\n",
    "  # Build selected model\n",
    "  loss, train_op, X_time_orig, X_time_recon, X_features_orig, X_features_recon = \\\n",
    "    model_function(X, mode, params, cur_batch_size, num_features, dummy_variable)\n",
    "  \n",
    "  if not (mode == tf.estimator.ModeKeys.TRAIN and params[\"evaluation_mode\"] == \"reconstruction\"):\n",
    "    # shape = (cur_batch_size * seq_len, num_features)\n",
    "    X_time_abs_recon_err = tf.abs(\n",
    "      x = X_time_orig - X_time_recon)\n",
    "\n",
    "    # Features based\n",
    "    # shape = (cur_batch_size * num_features, seq_len)\n",
    "    X_features_abs_recon_err = tf.abs(\n",
    "      x = X_features_orig - X_features_recon)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN and params[\"evaluation_mode\"] == \"calculate_error_distribution_statistics\":\n",
    "      ################################################################################\n",
    "\n",
    "      with tf.variable_scope(name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "        # Time based ########################################\n",
    "        singleton_time_condition = tf.equal(\n",
    "          x = cur_batch_size * params[\"seq_len\"], y = 1) # shape = ()\n",
    "        \n",
    "        cov_time_variable, mean_time_variable, count_time_variable = tf.cond(\n",
    "          pred = singleton_time_condition, \n",
    "          true_fn = lambda: singleton_batch_cov_variable_updating(\n",
    "            params[\"seq_len\"], \n",
    "            X_time_abs_recon_err, \n",
    "            abs_err_count_time_variable, \n",
    "            abs_err_mean_time_variable, \n",
    "            abs_err_cov_time_variable,\n",
    "            params[\"eps\"]), \n",
    "          false_fn = lambda: non_singleton_batch_cov_variable_updating(\n",
    "            cur_batch_size, \n",
    "            params[\"seq_len\"], \n",
    "            X_time_abs_recon_err, \n",
    "            abs_err_count_time_variable, \n",
    "            abs_err_mean_time_variable, \n",
    "            abs_err_cov_time_variable,\n",
    "            params[\"eps\"]))\n",
    "\n",
    "        # Features based ########################################\n",
    "        singleton_features_condition = tf.equal(\n",
    "          x = cur_batch_size * num_features, y = 1) # shape = ()\n",
    "        \n",
    "        cov_features_variable, mean_features_variable, count_features_variable = tf.cond(\n",
    "          pred = singleton_features_condition, \n",
    "          true_fn = lambda: singleton_batch_cov_variable_updating(\n",
    "            num_features, \n",
    "            X_features_abs_recon_err, \n",
    "            abs_err_count_features_variable, \n",
    "            abs_err_mean_features_variable, \n",
    "            abs_err_cov_features_variable,\n",
    "            params[\"eps\"]), \n",
    "          false_fn = lambda: non_singleton_batch_cov_variable_updating(\n",
    "            cur_batch_size, \n",
    "            num_features, \n",
    "            X_features_abs_recon_err, \n",
    "            abs_err_count_features_variable, \n",
    "            abs_err_mean_features_variable, \n",
    "            abs_err_cov_features_variable,\n",
    "            params[\"eps\"]))\n",
    "\n",
    "      # Lastly use control dependencies around loss to enforce the mahalanobis variables to be assigned, the control order matters, hence the separate contexts\n",
    "      with tf.control_dependencies(\n",
    "        control_inputs = [cov_time_variable, cov_features_variable]):\n",
    "        with tf.control_dependencies(\n",
    "          control_inputs = [mean_time_variable, mean_features_variable]):\n",
    "          with tf.control_dependencies(\n",
    "            control_inputs = [count_time_variable, count_features_variable]):\n",
    "            # Time based\n",
    "            # shape = (num_features, num_features)\n",
    "            abs_err_inv_cov_time_tensor = \\\n",
    "              tf.matrix_inverse(input = cov_time_variable + \\\n",
    "                tf.eye(num_rows = tf.shape(input = cov_time_variable)[0], \n",
    "                     dtype = tf.float64) * params[\"eps\"])\n",
    "            # Features based\n",
    "            # shape = (seq_len, seq_len)\n",
    "            abs_err_inv_cov_features_tensor = \\\n",
    "              tf.matrix_inverse(input = cov_features_variable + \\\n",
    "                tf.eye(num_rows = tf.shape(input = cov_features_variable)[0], \n",
    "                     dtype = tf.float64) * params[\"eps\"])\n",
    "            \n",
    "            with tf.control_dependencies(\n",
    "              control_inputs = [tf.assign(ref = abs_err_inv_cov_time_variable, value = abs_err_inv_cov_time_tensor), \n",
    "                                tf.assign(ref = abs_err_inv_cov_features_variable, value = abs_err_inv_cov_features_tensor)]):\n",
    "              loss = tf.reduce_sum(input_tensor = tf.zeros(shape = (), dtype = tf.float64) * dummy_variable)\n",
    "\n",
    "              train_op = tf.contrib.layers.optimize_loss(\n",
    "                loss = loss,\n",
    "                global_step = tf.train.get_global_step(),\n",
    "                learning_rate = params[\"learning_rate\"],\n",
    "                optimizer = \"SGD\")\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL and params[\"evaluation_mode\"] != \"tune_anomaly_thresholds\":\n",
    "      # Reconstruction loss on evaluation set\n",
    "      loss = tf.losses.mean_squared_error(labels = X_time_orig, predictions = X_time_recon)\n",
    "\n",
    "      if params[\"evaluation_mode\"] == \"reconstruction\":\n",
    "        # Reconstruction eval metrics\n",
    "        eval_metric_ops = {\n",
    "          \"rmse\": tf.metrics.root_mean_squared_error(labels = X_time_orig, predictions = X_time_recon),\n",
    "          \"mae\": tf.metrics.mean_absolute_error(labels = X_time_orig, predictions = X_time_recon)\n",
    "        }\n",
    "    elif mode == tf.estimator.ModeKeys.PREDICT or ((mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL) and params[\"evaluation_mode\"] == \"tune_anomaly_thresholds\"):\n",
    "      with tf.variable_scope(name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "        # Time based\n",
    "        mahalanobis_distance_time = mahalanobis_distance(\n",
    "          error_vectors_reshaped = X_time_abs_recon_err,\n",
    "          mean_vector = abs_err_mean_time_variable, \n",
    "          inv_covariance = abs_err_inv_cov_time_variable, \n",
    "          final_shape = params[\"seq_len\"]) # shape = (cur_batch_size, seq_len)\n",
    "        \n",
    "        # Features based\n",
    "        mahalanobis_distance_features = mahalanobis_distance(\n",
    "          error_vectors_reshaped = X_features_abs_recon_err,\n",
    "          mean_vector = abs_err_mean_features_variable, \n",
    "          inv_covariance = abs_err_inv_cov_features_variable,\n",
    "          final_shape = num_features) # shape = (cur_batch_size, num_features)\n",
    "\n",
    "      if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        labels_normal_mask = tf.equal(x = labels, y = 0)\n",
    "        labels_anomalous_mask = tf.equal(x = labels, y = 1)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "          with tf.variable_scope(\n",
    "            name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "            # Time based\n",
    "            # shape = (num_time_anomaly_thresholds,)\n",
    "            time_anomaly_thresholds = tf.linspace(\n",
    "              start = tf.constant(value = params[\"min_time_anomaly_threshold\"], dtype = tf.float64),\n",
    "              stop = tf.constant(value = params[\"max_time_anomaly_threshold\"], dtype = tf.float64), \n",
    "              num = params[\"num_time_anomaly_thresholds\"])\n",
    "\n",
    "            tp_time_update_op, fn_time_update_op, fp_time_update_op, tn_time_update_op = \\\n",
    "              update_anomaly_threshold_variables(\n",
    "                labels_normal_mask, \n",
    "                labels_anomalous_mask, \n",
    "                params[\"num_time_anomaly_thresholds\"], \n",
    "                time_anomaly_thresholds, \n",
    "                mahalanobis_distance_time, \n",
    "                tp_at_thresholds_time_variable, \n",
    "                fn_at_thresholds_time_variable, \n",
    "                fp_at_thresholds_time_variable, \n",
    "                tn_at_thresholds_time_variable,\n",
    "                mode)\n",
    "\n",
    "            # Features based\n",
    "            # shape = (num_features_anomaly_thresholds,)\n",
    "            features_anomaly_thresholds = tf.linspace(\n",
    "              start = tf.constant(value = params[\"min_features_anomaly_threshold\"], dtype = tf.float64),\n",
    "              stop = tf.constant(value = params[\"max_features_anomaly_threshold\"], dtype = tf.float64), \n",
    "              num = params[\"num_features_anomaly_thresholds\"])\n",
    "\n",
    "            tp_features_update_op, fn_features_update_op, fp_features_update_op, tn_features_update_op = \\\n",
    "              update_anomaly_threshold_variables(\n",
    "                labels_normal_mask, \n",
    "                labels_anomalous_mask, \n",
    "                params[\"num_features_anomaly_thresholds\"], \n",
    "                features_anomaly_thresholds, \n",
    "                mahalanobis_distance_features, \n",
    "                tp_at_thresholds_features_variable, \n",
    "                fn_at_thresholds_features_variable, \n",
    "                fp_at_thresholds_features_variable, \n",
    "                tn_at_thresholds_features_variable, \n",
    "                mode)\n",
    "\n",
    "          # Reconstruction loss on evaluation set\n",
    "          with tf.control_dependencies(\n",
    "            control_inputs = [\n",
    "              tp_time_update_op, \n",
    "              fn_time_update_op, \n",
    "              fp_time_update_op, \n",
    "              tn_time_update_op, \n",
    "              tp_features_update_op, \n",
    "              fn_features_update_op, \n",
    "              fp_features_update_op, \n",
    "              tn_features_update_op]):\n",
    "            # Time based\n",
    "            acc_time, pre_time, rec_time, f_beta_score_time = \\\n",
    "              calculate_composite_classification_metrics(\n",
    "                time_anomaly_thresholds, \n",
    "                tp_at_thresholds_time_variable, \n",
    "                fn_at_thresholds_time_variable, \n",
    "                fp_at_thresholds_time_variable, \n",
    "                tn_at_thresholds_time_variable,\n",
    "                params[\"f_score_beta\"])\n",
    "\n",
    "            # Features based\n",
    "            acc_features, pre_features, rec_features, f_beta_score_features = \\\n",
    "              calculate_composite_classification_metrics(\n",
    "                features_anomaly_thresholds, \n",
    "                tp_at_thresholds_features_variable, \n",
    "                fn_at_thresholds_features_variable, \n",
    "                fp_at_thresholds_features_variable, \n",
    "                tn_at_thresholds_features_variable,\n",
    "                params[\"f_score_beta\"])\n",
    "\n",
    "            with tf.control_dependencies(\n",
    "              control_inputs = [pre_time, pre_features]):\n",
    "              with tf.control_dependencies(\n",
    "                control_inputs = [rec_time, rec_features]):\n",
    "                with tf.control_dependencies(\n",
    "                  control_inputs = [f_beta_score_time, f_beta_score_features]):\n",
    "                  # Time based\n",
    "                  best_anomaly_threshold_time = find_best_anomaly_threshold(\n",
    "                    time_anomaly_thresholds, \n",
    "                    f_beta_score_time, \n",
    "                    params[\"time_anomaly_threshold\"], \n",
    "                    time_anomaly_threshold_variable)\n",
    "\n",
    "                  # Features based\n",
    "                  best_anomaly_threshold_features = find_best_anomaly_threshold(\n",
    "                    features_anomaly_thresholds, \n",
    "                    f_beta_score_features, \n",
    "                    params[\"features_anomaly_threshold\"], \n",
    "                    features_anomaly_threshold_variable)\n",
    "\n",
    "                  with tf.control_dependencies(\n",
    "                    control_inputs = [\n",
    "                      tf.assign(\n",
    "                        ref = time_anomaly_threshold_variable, \n",
    "                        value = best_anomaly_threshold_time), \n",
    "                      tf.assign(ref = \n",
    "                                features_anomaly_threshold_variable, \n",
    "                                value = best_anomaly_threshold_features)]):\n",
    "\n",
    "                    loss = tf.reduce_sum(\n",
    "                      input_tensor = tf.zeros(shape = (), dtype = tf.float64) * dummy_variable)\n",
    "\n",
    "                    train_op = tf.contrib.layers.optimize_loss(\n",
    "                      loss = loss,\n",
    "                      global_step = tf.train.get_global_step(),\n",
    "                      learning_rate = params[\"learning_rate\"],\n",
    "                      optimizer = \"SGD\")\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "          with tf.variable_scope(\n",
    "            name_or_scope = \"anomaly_threshold_eval_variables\", reuse = tf.AUTO_REUSE):\n",
    "            # Time based\n",
    "            tp_time_update_op, fn_time_update_op, fp_time_update_op, tn_time_update_op = \\\n",
    "              update_anomaly_threshold_variables(\n",
    "                labels_normal_mask, \n",
    "                labels_anomalous_mask, \n",
    "                1,\n",
    "                time_anomaly_threshold_variable, \n",
    "                mahalanobis_distance_time, \n",
    "                tp_at_threshold_eval_time_variable, \n",
    "                fn_at_threshold_eval_time_variable, \n",
    "                fp_at_threshold_eval_time_variable, \n",
    "                tn_at_threshold_eval_time_variable,\n",
    "                mode)\n",
    "\n",
    "            # Features based\n",
    "            tp_features_update_op, fn_features_update_op, fp_features_update_op, tn_features_update_op = \\\n",
    "              update_anomaly_threshold_variables(\n",
    "                labels_normal_mask, \n",
    "                labels_anomalous_mask, \n",
    "                1,\n",
    "                features_anomaly_threshold_variable, \n",
    "                mahalanobis_distance_features, \n",
    "                tp_at_threshold_eval_features_variable, \n",
    "                fn_at_threshold_eval_features_variable, \n",
    "                fp_at_threshold_eval_features_variable, \n",
    "                tn_at_threshold_eval_features_variable,\n",
    "                mode)\n",
    "\n",
    "          with tf.variable_scope(\n",
    "            name_or_scope = \"anomaly_threshold_eval_variables\", reuse = tf.AUTO_REUSE):\n",
    "            # Time based\n",
    "            acc_time_update_op, pre_time_update_op, rec_time_update_op, f_beta_score_time_update_op = \\\n",
    "              calculate_composite_classification_metrics(\n",
    "                time_anomaly_threshold_variable, \n",
    "                tp_at_threshold_eval_time_variable, \n",
    "                fn_at_threshold_eval_time_variable, \n",
    "                fp_at_threshold_eval_time_variable, \n",
    "                tn_at_threshold_eval_time_variable,\n",
    "                params[\"f_score_beta\"]) \n",
    "\n",
    "            # Features based\n",
    "            acc_features_update_op, pre_features_update_op, rec_features_update_op, f_beta_score_features_update_op = \\\n",
    "              calculate_composite_classification_metrics(\n",
    "                features_anomaly_threshold_variable, \n",
    "                tp_at_threshold_eval_features_variable, \n",
    "                fn_at_threshold_eval_features_variable, \n",
    "                fp_at_threshold_eval_features_variable, \n",
    "                tn_at_threshold_eval_features_variable,\n",
    "                params[\"f_score_beta\"]) \n",
    "\n",
    "          loss = tf.losses.mean_squared_error(labels = X_time_orig, predictions = X_time_recon)\n",
    "\n",
    "          acc_at_threshold_eval_time_variable = tf.cast(x = tp_at_threshold_eval_time_variable + tn_at_threshold_eval_time_variable, dtype = tf.float64) \\\n",
    "            / tf.cast(x = tp_at_threshold_eval_time_variable + fn_at_threshold_eval_time_variable + fp_at_threshold_eval_time_variable + tn_at_threshold_eval_time_variable, dtype = tf.float64)\n",
    "          pre_at_threshold_eval_time_variable = tf.cast(x = tp_at_threshold_eval_time_variable, dtype = tf.float64) \\\n",
    "            / tf.cast(x = tp_at_threshold_eval_time_variable + fp_at_threshold_eval_time_variable, dtype = tf.float64)\n",
    "          rec_at_threshold_eval_time_variable = tf.cast(x = tp_at_threshold_eval_time_variable, dtype = tf.float64) \\\n",
    "            / tf.cast(x = tp_at_threshold_eval_time_variable + fn_at_threshold_eval_time_variable, dtype = tf.float64)\n",
    "          f_beta_score_at_threshold_eval_time_variable = (1.0 + params[\"f_score_beta\"] ** 2) * pre_at_threshold_eval_time_variable * rec_at_threshold_eval_time_variable \\\n",
    "            / (params[\"f_score_beta\"] ** 2 * pre_at_threshold_eval_time_variable + rec_at_threshold_eval_time_variable)\n",
    "\n",
    "          acc_at_threshold_eval_features_variable = tf.cast(x = tp_at_threshold_eval_features_variable + tn_at_threshold_eval_features_variable, dtype = tf.float64) \\\n",
    "            / tf.cast(x = tp_at_threshold_eval_features_variable + fn_at_threshold_eval_features_variable + fp_at_threshold_eval_features_variable + tn_at_threshold_eval_features_variable, dtype = tf.float64)\n",
    "          pre_at_threshold_eval_features_variable = tf.cast(x = tp_at_threshold_eval_features_variable, dtype = tf.float64) \\\n",
    "            / tf.cast(x = tp_at_threshold_eval_features_variable + fp_at_threshold_eval_features_variable, dtype = tf.float64)\n",
    "          rec_at_threshold_eval_features_variable = tf.cast(x = tp_at_threshold_eval_features_variable, dtype = tf.float64) \\\n",
    "            / tf.cast(x = tp_at_threshold_eval_features_variable + fn_at_threshold_eval_features_variable, dtype = tf.float64)\n",
    "          f_beta_score_at_threshold_eval_features_variable = (1.0 + params[\"f_score_beta\"] ** 2) * pre_at_threshold_eval_features_variable * rec_at_threshold_eval_features_variable \\\n",
    "            / (params[\"f_score_beta\"] ** 2 * pre_at_threshold_eval_features_variable + rec_at_threshold_eval_features_variable)\n",
    "\n",
    "          # Anomaly detection eval metrics\n",
    "          eval_metric_ops = {\n",
    "            # Time based\n",
    "            \"time_anomaly_tp\": (tp_at_threshold_eval_time_variable, tp_time_update_op),\n",
    "            \"time_anomaly_fn\": (fn_at_threshold_eval_time_variable, fn_time_update_op),\n",
    "            \"time_anomaly_fp\": (fp_at_threshold_eval_time_variable, fp_time_update_op),\n",
    "            \"time_anomaly_tn\": (tn_at_threshold_eval_time_variable, tn_time_update_op),\n",
    "\n",
    "            \"time_anomaly_acc\": (acc_at_threshold_eval_time_variable, acc_time_update_op),\n",
    "            \"time_anomaly_pre\": (pre_at_threshold_eval_time_variable, pre_time_update_op),\n",
    "            \"time_anomaly_rec\": (rec_at_threshold_eval_time_variable, rec_time_update_op),\n",
    "            \"time_anomaly_f_beta_score\": (f_beta_score_at_threshold_eval_time_variable, f_beta_score_time_update_op),\n",
    "\n",
    "             # Features based\n",
    "            \"features_anomaly_tp\": (tp_at_threshold_eval_features_variable, tp_features_update_op),\n",
    "            \"features_anomaly_fn\": (fn_at_threshold_eval_features_variable, fn_features_update_op),\n",
    "            \"features_anomaly_fp\": (fp_at_threshold_eval_features_variable, fp_features_update_op),\n",
    "            \"features_anomaly_tn\": (tn_at_threshold_eval_features_variable, tn_features_update_op),\n",
    "\n",
    "            \"features_anomaly_acc\": (acc_at_threshold_eval_features_variable, acc_features_update_op),\n",
    "            \"features_anomaly_pre\": (pre_at_threshold_eval_features_variable, pre_features_update_op),\n",
    "            \"features_anomaly_rec\": (rec_at_threshold_eval_features_variable, rec_features_update_op),\n",
    "            \"features_anomaly_f_beta_score\": (f_beta_score_at_threshold_eval_features_variable, f_beta_score_features_update_op)\n",
    "          }\n",
    "      else: # mode == tf.estimator.ModeKeys.PREDICT\n",
    "        # Flag predictions as either normal or anomalous\n",
    "        time_anomaly_flags = tf.where(\n",
    "          condition = tf.reduce_any(\n",
    "            input_tensor = tf.greater(\n",
    "              x = tf.abs(x = mahalanobis_distance_time),\n",
    "              y = time_anomaly_threshold_variable), \n",
    "            axis = 1), \n",
    "          x = tf.ones(shape = [cur_batch_size], dtype = tf.int64), \n",
    "          y = tf.zeros(shape = [cur_batch_size], dtype = tf.int64)) # shape = (cur_batch_size,)\n",
    "\n",
    "        features_anomaly_flags = tf.where(\n",
    "          condition = tf.reduce_any(\n",
    "            input_tensor = tf.greater(\n",
    "              x = tf.abs(x = mahalanobis_distance_features),\n",
    "              y = features_anomaly_threshold_variable), \n",
    "            axis = 1), \n",
    "          x = tf.ones(shape = [cur_batch_size], dtype = tf.int64), \n",
    "          y = tf.zeros(shape = [cur_batch_size], dtype = tf.int64)) # shape = (cur_batch_size,)\n",
    "\n",
    "        # Create predictions dictionary\n",
    "        predictions_dict = {\n",
    "          \"X_time_abs_recon_err\": tf.reshape(\n",
    "            tensor = X_time_abs_recon_err, \n",
    "            shape = [cur_batch_size, params[\"seq_len\"], num_features]), \n",
    "          \"X_features_abs_recon_err\": tf.transpose(\n",
    "            a = tf.reshape(\n",
    "              tensor = X_features_abs_recon_err, \n",
    "              shape = [cur_batch_size, num_features, params[\"seq_len\"]]), \n",
    "            perm = [0, 2, 1]),\n",
    "          \"mahalanobis_distance_time\": mahalanobis_distance_time, \n",
    "          \"mahalanobis_distance_features\": mahalanobis_distance_features, \n",
    "          \"time_anomaly_flags\": time_anomaly_flags, \n",
    "          \"features_anomaly_flags\": features_anomaly_flags}\n",
    "\n",
    "        # Create export outputs\n",
    "        export_outputs = {\n",
    "          \"predict_export_outputs\": tf.estimator.export.PredictOutput(\n",
    "            outputs = predictions_dict)}\n",
    "\n",
    "  # Return EstimatorSpec\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "    mode = mode,\n",
    "    predictions = predictions_dict,\n",
    "    loss = loss,\n",
    "    train_op = train_op,\n",
    "    eval_metric_ops = eval_metric_ops,\n",
    "    export_outputs = export_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our serving input function to accept the data at serving and send it in the \n",
    "# right format to our custom estimator\n",
    "def serving_input_fn(seq_len):\n",
    "    # This function fixes the shape and type of our input strings\n",
    "    def fix_shape_and_type_for_serving(placeholder):\n",
    "        current_batch_size = tf.shape(input = placeholder, out_type = tf.int64)[0]\n",
    "        \n",
    "        # String split each string in batch and output values from the resulting SparseTensors\n",
    "        split_string = tf.stack(values = tf.map_fn( # shape = (batch_size, seq_len)\n",
    "            fn = lambda x: tf.string_split(source = [placeholder[x]], delimiter = ',').values, \n",
    "            elems = tf.range(start = 0, limit = current_batch_size, dtype = tf.int64), \n",
    "            dtype = tf.string), axis = 0)\n",
    "        \n",
    "        # Convert each string in the split tensor to float\n",
    "        # shape = (batch_size, seq_len)\n",
    "        feature_tensor = tf.string_to_number(string_tensor = split_string, out_type = tf.float64)\n",
    "        \n",
    "        return feature_tensor\n",
    "    \n",
    "    # This function fixes dynamic shape ambiguity of last dimension so that we will be able to \n",
    "    # use it in our DNN (since tf.layers.dense require the last dimension to be known)\n",
    "    def get_shape_and_set_modified_shape_2D(tensor, additional_dimension_sizes):\n",
    "        # Get static shape for tensor and convert it to list\n",
    "        shape = tensor.get_shape().as_list()\n",
    "        # Set outer shape to additional_dimension_sizes[0] since know this is the correct size\n",
    "        shape[1] = additional_dimension_sizes[0]\n",
    "        # Set the shape of tensor to our modified shape\n",
    "        tensor.set_shape(shape = shape) # shape = (batch_size, additional_dimension_sizes[0])\n",
    "\n",
    "        return tensor\n",
    "            \n",
    "    # Create placeholders to accept the data sent to the model at serving time\n",
    "    # All features come in as a batch of strings, shape = (batch_size,), \n",
    "    # this was so because of passing the arrays to online ml-engine prediction\n",
    "    feature_placeholders = {\n",
    "        feature: tf.placeholder(\n",
    "          dtype = tf.string, shape = [None]) for feature in UNLABELED_CSV_COLUMNS\n",
    "    }\n",
    "    \n",
    "    # Create feature tensors\n",
    "    features = {key: fix_shape_and_type_for_serving(placeholder = tensor) \n",
    "      for key, tensor in feature_placeholders.items()}\n",
    "    \n",
    "    # Fix dynamic shape ambiguity of feature tensors for our DNN\n",
    "    features = {key: get_shape_and_set_modified_shape_2D(\n",
    "      tensor = tensor, additional_dimension_sizes = [seq_len]) for key, tensor in features.items()}\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "      features = features, receiver_tensors = feature_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator to train and evaluate\n",
    "def train_and_evaluate(args):\n",
    "  # Create our custom estimator using our model function\n",
    "  estimator = tf.estimator.Estimator(\n",
    "    model_fn = anomaly_detection,\n",
    "    model_dir = args[\"output_dir\"],\n",
    "    params = {key: val for key, val in args.items()})\n",
    "  \n",
    "  if args[\"evaluation_mode\"] == \"reconstruction\":\n",
    "    estimator.train(\n",
    "      input_fn = read_dataset(\n",
    "        filename = args[\"train_file_pattern\"], \n",
    "        mode = tf.estimator.ModeKeys.EVAL, \n",
    "        batch_size = args[\"train_batch_size\"],\n",
    "        params = args),\n",
    "      steps = None)\n",
    "  else:\n",
    "    if args[\"evaluation_mode\"] == \"calculate_error_distribution_statistics\":\n",
    "      # Get final mahalanobis statistics over the entire validation_1 dataset\n",
    "      estimator.train(\n",
    "        input_fn = read_dataset(\n",
    "          filename = args[\"train_file_pattern\"], \n",
    "          mode = tf.estimator.ModeKeys.EVAL, \n",
    "          batch_size = args[\"train_batch_size\"],\n",
    "          params = args),\n",
    "        steps = None)\n",
    "\n",
    "    elif args[\"evaluation_mode\"] == \"tune_anomaly_thresholds\":\n",
    "      # Tune anomaly thresholds using valdiation_2 and validation_anomaly datasets\n",
    "      estimator.train(\n",
    "        input_fn = read_dataset(\n",
    "          filename = args[\"train_file_pattern\"], \n",
    "          mode = tf.estimator.ModeKeys.EVAL, \n",
    "          batch_size = args[\"train_batch_size\"],\n",
    "          params = args),\n",
    "        steps = None)\n",
    "      \n",
    "      estimator.evaluate(\n",
    "        input_fn = read_dataset(\n",
    "          filename = args[\"eval_file_pattern\"], \n",
    "          mode = tf.estimator.ModeKeys.EVAL, \n",
    "          batch_size = args[\"eval_batch_size\"],\n",
    "          params = args),\n",
    "        steps = None)\n",
    "\n",
    "    # Export savedmodel with learned error distribution statistics to be used for inference\n",
    "    estimator.export_savedmodel(\n",
    "      export_dir_base = args['output_dir'] + \"/export/exporter\", \n",
    "      serving_input_receiver_fn = lambda: serving_input_fn(args[\"seq_len\"]))\n",
    "    \n",
    "  return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {}\n",
    "# File arguments\n",
    "arguments[\"train_file_pattern\"] = \"data/training_normal_sequences.csv\"\n",
    "arguments[\"eval_file_pattern\"] = \"data/validation_normal_1_sequences.csv\"\n",
    "arguments[\"output_dir\"] = \"trained_model\"\n",
    "\n",
    "# Sequence shape hyperparameters\n",
    "arguments[\"seq_len\"] = seq_len\n",
    "\n",
    "# Training parameters\n",
    "arguments[\"train_batch_size\"] = 32\n",
    "arguments[\"eval_batch_size\"] = 32\n",
    "arguments[\"train_steps\"] = 2000\n",
    "arguments[\"learning_rate\"] = 0.01\n",
    "arguments[\"start_delay_secs\"] = 60\n",
    "arguments[\"throttle_secs\"] = 120\n",
    "\n",
    "# Model parameters\n",
    "# [dense_autoencoder, lstm_encoder_decoder_autoencoder, pca]\n",
    "arguments[\"model_type\"] = \"lstm_encoder_decoder_autoencoder\"\n",
    "\n",
    "## Dense Autoencoder\n",
    "arguments[\"encoder_dnn_hidden_units\"] = [64, 32, 16]\n",
    "arguments[\"latent_vector_size\"] = min(8, arguments[\"encoder_dnn_hidden_units\"][-1])\n",
    "arguments[\"decoder_dnn_hidden_units\"] = [16, 32, 64]\n",
    "arguments[\"time_loss_weight\"] = 1.0\n",
    "arguments[\"features_loss_weight\"] = 1.0\n",
    "\n",
    "## LSTM Encoder-Decoder Autoencoder\n",
    "arguments[\"reverse_labels_sequence\"] = True\n",
    "### LSTM hyperparameters\n",
    "arguments[\"encoder_lstm_hidden_units\"] = [64, 32, 16]\n",
    "arguments[\"decoder_lstm_hidden_units\"] = [16, 32, 64]\n",
    "arguments[\"lstm_dropout_output_keep_probs\"] = [1.0, 1.0, 1.0]\n",
    "### DNN hyperparameters\n",
    "arguments[\"dnn_hidden_units\"] = [1024, 256, 64]\n",
    "\n",
    "## PCA\n",
    "arguments[\"k_principal_components\"] = min(number_of_tags, 3)\n",
    "\n",
    "# Anomaly detection\n",
    "arguments[\"evaluation_mode\"] = \"reconstruction\"\n",
    "arguments[\"num_time_anomaly_thresholds\"] = 300\n",
    "arguments[\"num_features_anomaly_thresholds\"] = 300\n",
    "arguments[\"min_time_anomaly_threshold\"] = 1\n",
    "arguments[\"max_time_anomaly_threshold\"] = 100\n",
    "arguments[\"min_features_anomaly_threshold\"] = 1\n",
    "arguments[\"max_features_anomaly_threshold\"] = 100\n",
    "arguments[\"time_anomaly_threshold\"] = None\n",
    "arguments[\"features_anomaly_threshold\"] = None\n",
    "arguments[\"eps\"] = 10**-12\n",
    "arguments[\"f_score_beta\"] = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train reconstruction variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "shutil.rmtree(path = arguments[\"output_dir\"], ignore_errors = True) # start fresh each time\n",
    "estimator = train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at any special variable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pca_variables(X, dim_name):\n",
    "  print(\"X_{}.shape = \\n{}\".format(dim_name, X.shape))\n",
    "  \n",
    "  # Count\n",
    "  count_var = estimator.get_variable_value(name = \"pca_variables/pca_{}_count_variable\".format(dim_name))\n",
    "  print(\"{}_count_var = \\n{}\".format(dim_name, count_var))\n",
    "  count = X.shape[0]\n",
    "  print(\"{}_count = \\n{}\".format(dim_name, count))\n",
    "\n",
    "  # Mean\n",
    "  mean_var = estimator.get_variable_value(name = \"pca_variables/pca_{}_mean_variable\".format(dim_name))\n",
    "  print(\"{}_mean_var = \\n{}\".format(dim_name, mean_var))\n",
    "  mean = np.mean(X, axis = 0)\n",
    "  print(\"{}_mean = \\n{}\".format(dim_name, mean))\n",
    "  print(\"{}_mean_ratio = \\n{}\".format(dim_name, mean_var / mean))\n",
    "  \n",
    "  # Covariance\n",
    "  cov_var = estimator.get_variable_value(name = \"pca_variables/pca_{}_cov_variable\".format(dim_name))\n",
    "  if cov_var.shape[0] <= 10:\n",
    "    print(\"{}_cov_var = \\n{}\".format(dim_name, cov_var))\n",
    "  else:\n",
    "    print(\"{}_cov_var.shape = \\n{}\".format(dim_name, cov_var.shape))\n",
    "    \n",
    "  if arguments[\"seq_len\"] == 1:\n",
    "    cov = np.zeros(shape = [number_of_tags, number_of_tags])\n",
    "  else:\n",
    "    cov = np.cov(np.transpose(X))\n",
    "  if cov.shape[0] <= 10:\n",
    "    print(\"{}_cov = \\n{}\".format(dim_name, cov))\n",
    "  else:\n",
    "    print(\"{}_cov.shape = \\n{}\".format(dim_name, cov.shape))\n",
    "    \n",
    "  print(\"{}_cov_ratio = \\n{}\".format(dim_name, cov_var / cov))\n",
    "  \n",
    "  # Eigenvalues\n",
    "  eigenvalues_var = estimator.get_variable_value(name = \"pca_variables/pca_{}_eigenvalues_variable\".format(dim_name))\n",
    "  if eigenvalues_var.shape[0] <= 10:\n",
    "    print(\"{}_eigenvalues_var = \\n{}\".format(dim_name, eigenvalues_var))\n",
    "  else:\n",
    "    print(\"{}_eigenvalues_var.shape = \\n{}\".format(dim_name, eigenvalues_var.shape))\n",
    "    \n",
    "  eigenvalues, eigenvectors = np.linalg.eigh(a = cov)\n",
    "  if eigenvalues.shape[0] <= 10:\n",
    "    print(\"{}_eigenvalues = \\n{}\".format(dim_name, eigenvalues))\n",
    "  else:\n",
    "    print(\"{}_eigenvalues.shape = \\n{}\".format(dim_name, eigenvalues.shape))\n",
    "    \n",
    "  print(\"{}_eigenvalues_ratio = \\n{}\".format(dim_name, eigenvalues_var / eigenvalues))\n",
    "  \n",
    "  # Eigenvectors\n",
    "  eigenvectors_var = estimator.get_variable_value(name = \"pca_variables/pca_{}_eigenvectors_variable\".format(dim_name))\n",
    "  if eigenvectors_var.shape[0] <= 10:\n",
    "    print(\"{}_eigenvectors_var = \\n{}\".format(dim_name, eigenvectors_var))\n",
    "  else:\n",
    "    print(\"{}_eigenvectors_var.shape = \\n{}\".format(dim_name, eigenvectors_var.shape))\n",
    "    \n",
    "  if eigenvectors.shape[0] <= 10:\n",
    "    print(\"{}_eigenvectors = \\n{}\".format(dim_name, eigenvectors))\n",
    "  else:\n",
    "    print(\"{}_eigenvectors.shape = \\n{}\".format(dim_name, eigenvectors.shape))\n",
    "    \n",
    "  print(\"{}_eigenvectors_ratio = \\n{}\".format(dim_name, eigenvectors_var / eigenvectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if arguments[\"model_type\"] == \"pca\":\n",
    "  print(\"Getting PCA data:\\n\")\n",
    "  arr_training_normal_sequences = np.genfromtxt(\n",
    "    fname = \"data/training_normal_sequences.csv\", delimiter = ';', dtype = str)\n",
    "  print(\"arr_training_normal_sequences.shape = {}\".format(arr_training_normal_sequences.shape))\n",
    "  if number_of_tags == 1:\n",
    "    arr_training_normal_sequences = np.expand_dims(a = arr_training_normal_sequences, axis = -1)\n",
    "\n",
    "  arr_training_normal_sequences_features = np.stack(\n",
    "    arrays = [np.stack(\n",
    "      arrays = [np.array(arr_training_normal_sequences[example_index, tag_index].split(',')).astype(np.float) \n",
    "                for tag_index in range(number_of_tags)], axis = 1) \n",
    "              for example_index in range(len(arr_training_normal_sequences))], axis = 0)\n",
    "\n",
    "  print(\"arr_training_normal_sequences_features.shape = {}\".format(arr_training_normal_sequences_features.shape))\n",
    "  \n",
    "  # Time based\n",
    "  X_time = arr_training_normal_sequences_features.reshape(arr_training_normal_sequences_features.shape[0] * arr_training_normal_sequences_features.shape[1], number_of_tags)\n",
    "  print(\"\\nPCA Time Variables:\\n\")\n",
    "  print_pca_variables(X_time, \"time\")\n",
    "  \n",
    "  # Features based\n",
    "  X_features = np.transpose(arr_training_normal_sequences_features, [0, 2, 1]).reshape(arr_training_normal_sequences_features.shape[0] * number_of_tags, arr_training_normal_sequences_features.shape[1])\n",
    "  print(\"\\nPCA Features Variables:\\n\")\n",
    "  print_pca_variables(X_features, \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train error distribution statistics variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments[\"evaluation_mode\"] = \"calculate_error_distribution_statistics\"\n",
    "arguments[\"train_file_pattern\"] = \"data/validation_normal_1_sequences.csv\"\n",
    "arguments[\"eval_file_pattern\"] = \"data/validation_normal_1_sequences.csv\"\n",
    "arguments[\"train_batch_size\"] = 32\n",
    "arguments[\"eval_batch_size\"] = 32\n",
    "estimator = train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at variable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_validation_normal_1_sequences = np.genfromtxt(\n",
    "  fname = \"data/validation_normal_1_sequences.csv\", \n",
    "  delimiter = ';', \n",
    "  dtype = str)\n",
    "print(\"arr_validation_normal_1_sequences.shape = {}\".format(arr_validation_normal_1_sequences.shape))\n",
    "if number_of_tags == 1:\n",
    "  arr_validation_normal_1_sequences = np.expand_dims(a = arr_validation_normal_1_sequences, axis = -1)\n",
    "  \n",
    "arr_validation_normal_1_sequences_features = np.stack(\n",
    "  arrays = [np.stack(\n",
    "    arrays = [np.array(arr_validation_normal_1_sequences[example_index, tag_index].split(',')).astype(np.float) \n",
    "              for tag_index in range(number_of_tags)], axis = 1) \n",
    "            for example_index in range(len(arr_validation_normal_1_sequences))], axis = 0)\n",
    "\n",
    "dict_validation_normal_1_sequences_features = {tag: arr_validation_normal_1_sequences_features[:, :, index] \n",
    "                                               for index, tag in enumerate(UNLABELED_CSV_COLUMNS)}\n",
    "\n",
    "validation_normal_1_predictions_list = [prediction for prediction in estimator.predict(\n",
    "  input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x = dict_validation_normal_1_sequences_features,\n",
    "    y = None,\n",
    "    batch_size = 32,\n",
    "    num_epochs = 1,\n",
    "    shuffle = False,\n",
    "    queue_capacity = 1000))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_normal_1_time_absolute_error = np.stack(arrays = [prediction[\"X_time_abs_recon_err\"] for prediction in validation_normal_1_predictions_list], axis = 0)\n",
    "time_abs_err = validation_normal_1_time_absolute_error.reshape(validation_normal_1_time_absolute_error.shape[0] * validation_normal_1_time_absolute_error.shape[1], number_of_tags)\n",
    "time_abs_err.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_count_time_variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_count = time_abs_err.shape[0]\n",
    "time_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_mean_time_variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_mean = np.mean(time_abs_err, axis = 0)\n",
    "time_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_mean_time_variable\") / time_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_cov_time_variable\").shape[0] <= 10:\n",
    "  print(estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_cov_time_variable\"))\n",
    "else:\n",
    "  print(estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_cov_time_variable\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if arguments[\"seq_len\"] == 1:\n",
    "  time_cov = np.zeros(shape = [number_of_tags, number_of_tags])\n",
    "else:\n",
    "  time_cov = np.cov(np.transpose(time_abs_err))\n",
    "if time_cov.shape[0] <= 10:\n",
    "  print(time_cov)\n",
    "else:\n",
    "  print(time_cov.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_cov_time_variable\") / time_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_inv_cov_time_variable\").shape[0] <= 10:\n",
    "  print(estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_inv_cov_time_variable\"))\n",
    "else:\n",
    "  print(estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_inv_cov_time_variable\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_inv = np.linalg.inv(time_cov + np.eye(number_of_tags) * arguments[\"eps\"])\n",
    "if time_inv.shape[0] <= 10:\n",
    "  print(time_inv)\n",
    "else:\n",
    "  print(time_inv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_inv_cov_time_variable\") / time_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_normal_1_features_absolute_error = np.stack(arrays = [prediction[\"X_features_abs_recon_err\"] for prediction in validation_normal_1_predictions_list], axis = 0)\n",
    "feat_abs_err = np.transpose(validation_normal_1_features_absolute_error, [0, 2, 1]).reshape(validation_normal_1_features_absolute_error.shape[0] * number_of_tags, validation_normal_1_features_absolute_error.shape[1])\n",
    "feat_abs_err.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_count_features_variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_count = feat_abs_err.shape[0]\n",
    "feat_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_mean_features_variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mean = np.mean(feat_abs_err, axis = 0)\n",
    "feat_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_mean_features_variable\") / feat_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_cov_features_variable\").shape[0] <= 10:\n",
    "  print(estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_cov_features_variable\"))\n",
    "else:\n",
    "  print(estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_cov_features_variable\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if number_of_tags == 1:\n",
    "  feat_cov = np.zeros(shape = [arguments[\"seq_len\"], arguments[\"seq_len\"]])\n",
    "else:\n",
    "  feat_cov = np.cov(np.transpose(feat_abs_err))\n",
    "if feat_cov.shape[0] <= 10:\n",
    "  print(feat_cov)\n",
    "else:\n",
    "  print(feat_cov.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_cov_features_variable\") / feat_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_inv_cov_features_variable\").shape[0] <= 10:\n",
    "  print(estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_inv_cov_features_variable\"))\n",
    "else:\n",
    "  print(estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_inv_cov_features_variable\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_inv = np.linalg.inv(feat_cov + np.eye(arguments[\"seq_len\"]) * arguments[\"eps\"])\n",
    "if feat_inv.shape[0] <= 10:\n",
    "  print(feat_inv)\n",
    "else:\n",
    "  print(feat_inv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_variables/abs_err_inv_cov_features_variable\") / feat_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune anomaly thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments[\"evaluation_mode\"] = \"tune_anomaly_thresholds\"\n",
    "arguments[\"train_file_pattern\"] = \"data/labeled_validation_mixed_sequences.csv\"\n",
    "arguments[\"eval_file_pattern\"] = \"data/labeled_validation_mixed_sequences.csv\"\n",
    "arguments[\"train_batch_size\"] = 64\n",
    "arguments[\"eval_batch_size\"] = 64\n",
    "estimator = train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_threshold_variables/tp_at_thresholds_time_variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_threshold_variables/fn_at_thresholds_time_variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_threshold_variables/fp_at_thresholds_time_variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_threshold_variables/tn_at_thresholds_time_variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_threshold_variables/time_anomaly_threshold_variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_threshold_variables/tp_at_thresholds_features_variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_threshold_variables/fn_at_thresholds_features_variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_threshold_variables/fp_at_thresholds_features_variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_threshold_variables/tn_at_thresholds_features_variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_variable_value(name = \"mahalanobis_distance_threshold_variables/features_anomaly_threshold_variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_validation_mixed_sequences = np.genfromtxt(\n",
    "  fname = \"data/labeled_validation_mixed_sequences.csv\", delimiter = ';', dtype = str)\n",
    "print(\"arr_validation_mixed_sequences.shape = {}\".format(arr_validation_mixed_sequences.shape))\n",
    "\n",
    "arr_validation_mixed_sequences_features = np.stack(\n",
    "  arrays = [np.stack(\n",
    "    arrays = [np.array(\n",
    "      object = arr_validation_mixed_sequences[example_index, tag_index].split(',')).astype(np.float) \n",
    "              for tag_index in range(number_of_tags)], axis = 1) \n",
    "            for example_index in range(len(arr_validation_mixed_sequences))], axis = 0)\n",
    "print(\"arr_validation_mixed_sequences_features.shape = {}\".format(arr_validation_mixed_sequences_features.shape))\n",
    "\n",
    "dict_validation_mixed_sequences_features = {tag: arr_validation_mixed_sequences_features[:, :, index] \n",
    "                                            for index, tag in enumerate(UNLABELED_CSV_COLUMNS)}\n",
    "\n",
    "validation_mixed_predictions_list = [prediction for prediction in estimator.predict(\n",
    "  input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x = dict_validation_mixed_sequences_features,\n",
    "    y = None,\n",
    "    batch_size = 128,\n",
    "    num_epochs = 1,\n",
    "    shuffle = False,\n",
    "    queue_capacity = 1000))]\n",
    "\n",
    "arr_validation_mixed_sequences_labels = arr_validation_mixed_sequences[:, -1].astype(np.float64)\n",
    "print(\"arr_validation_mixed_sequences_labels.shape = {}\".format(arr_validation_mixed_sequences_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_normal_mask = arr_validation_mixed_sequences_labels.astype(np.float64) == 0\n",
    "labels_anomalous_mask = arr_validation_mixed_sequences_labels.astype(np.float64) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_validation_mixed_predictions_mahalanobis_distance_batch_time = np.stack(\n",
    "  arrays = [prediction[\"mahalanobis_distance_time\"] \n",
    "            for prediction in validation_mixed_predictions_list], axis = 0)\n",
    "print(\"arr_validation_mixed_predictions_mahalanobis_distance_batch_time.shape = {}\".format(arr_validation_mixed_predictions_mahalanobis_distance_batch_time.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_normal_mahalanobis_distance_batch_time = np.min(np.max(\n",
    "  arr_validation_mixed_predictions_mahalanobis_distance_batch_time[labels_normal_mask, :], axis = -1))\n",
    "max_normal_mahalanobis_distance_batch_time = np.max(np.max(\n",
    "  arr_validation_mixed_predictions_mahalanobis_distance_batch_time[labels_normal_mask, :], axis = -1))\n",
    "print(\"min_normal_mahalanobis_distance_batch_time = {} & max_normal_mahalanobis_distance_batch_time = {}\".format(min_normal_mahalanobis_distance_batch_time, max_normal_mahalanobis_distance_batch_time))\n",
    "\n",
    "min_anomalous_mahalanobis_distance_batch_time = np.min(np.max(\n",
    "  arr_validation_mixed_predictions_mahalanobis_distance_batch_time[labels_anomalous_mask, :], axis = -1))\n",
    "max_anomalous_mahalanobis_distance_batch_time = np.max(np.max(\n",
    "  arr_validation_mixed_predictions_mahalanobis_distance_batch_time[labels_anomalous_mask, :], axis = -1))\n",
    "print(\"min_anomalous_mahalanobis_distance_batch_time = {} & max_anomalous_mahalanobis_distance_batch_time = {}\".format(min_anomalous_mahalanobis_distance_batch_time, max_anomalous_mahalanobis_distance_batch_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_anomaly_thresholds = arguments[\"num_time_anomaly_thresholds\"];\n",
    "batch_time_anomaly_thresholds = np.linspace(start = arguments[\"min_time_anomaly_threshold\"], \n",
    "                                            stop = arguments[\"max_time_anomaly_threshold\"], \n",
    "                                            num = num_time_anomaly_thresholds)\n",
    "print(\"batch_time_anomaly_thresholds.shape = {}\".format(batch_time_anomaly_thresholds.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_validation_mixed_predictions_mahalanobis_distance_batch_time_anomalies_multi_thresholds = np.stack(\n",
    "  arrays = [arr_validation_mixed_predictions_mahalanobis_distance_batch_time > batch_time_anomaly_threshold \n",
    "            for batch_time_anomaly_threshold in batch_time_anomaly_thresholds], axis = -1)\n",
    "print(\"arr_validation_mixed_predictions_mahalanobis_distance_batch_time_anomalies_multi_thresholds.shape = {}\".format(arr_validation_mixed_predictions_mahalanobis_distance_batch_time_anomalies_multi_thresholds.shape))\n",
    "arr_validation_mixed_predictions_mahalanobis_distance_batch_time_anomalies_multi_thresholds = np.any(\n",
    "  a = arr_validation_mixed_predictions_mahalanobis_distance_batch_time_anomalies_multi_thresholds, axis = 1)\n",
    "print(\"arr_validation_mixed_predictions_mahalanobis_distance_batch_time_anomalies_multi_thresholds.shape = {}\".format(arr_validation_mixed_predictions_mahalanobis_distance_batch_time_anomalies_multi_thresholds.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_normals = arr_validation_mixed_predictions_mahalanobis_distance_batch_time_anomalies_multi_thresholds == 0\n",
    "print(\"predicted_normals.shape = {}\".format(predicted_normals.shape))\n",
    "predicted_anomalies = arr_validation_mixed_predictions_mahalanobis_distance_batch_time_anomalies_multi_thresholds == 1\n",
    "print(\"predicted_anomalies.shape = {}\".format(predicted_anomalies.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = np.sum(\n",
    "  a = np.stack(arrays = [\n",
    "    np.logical_and(\n",
    "      labels_anomalous_mask, predicted_anomalies[:, threshold]) \n",
    "    for threshold in range(num_time_anomaly_thresholds)], \n",
    "               axis = -1), \n",
    "  axis = 0)\n",
    "\n",
    "false_negatives = np.sum(\n",
    "  a = np.stack(arrays = [\n",
    "    np.logical_and(\n",
    "      labels_anomalous_mask, predicted_normals[:, threshold]) \n",
    "    for threshold in range(num_time_anomaly_thresholds)], \n",
    "               axis = -1), \n",
    "  axis = 0)\n",
    "\n",
    "false_positives = np.sum(\n",
    "  a = np.stack(arrays = [\n",
    "    np.logical_and(\n",
    "      labels_normal_mask, predicted_anomalies[:, threshold]) \n",
    "    for threshold in range(num_time_anomaly_thresholds)], \n",
    "               axis = -1), \n",
    "  axis = 0)\n",
    "\n",
    "true_negatives = np.sum(\n",
    "  a = np.stack(arrays = [np.logical_and(\n",
    "    labels_normal_mask, predicted_normals[:, threshold]) \n",
    "                         for threshold in range(num_time_anomaly_thresholds)], \n",
    "               axis = -1), \n",
    "  axis = 0)\n",
    "print(\"true_positives.shape = {}, false_negatives.shape = {}, false_positives.shape = {}, true_negatives.shape = {}\".format(true_positives.shape, false_negatives.shape, false_positives.shape, true_negatives.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"true_positives = \\n{}\".format(true_positives))\n",
    "print(\"false_negatives = \\n{}\".format(false_negatives))\n",
    "print(\"false_positives = \\n{}\".format(false_positives))\n",
    "print(\"true_negatives = \\n{}\".format(true_negatives))\n",
    "print(\"true_positives + false_negatives + false_positives + true_negatives = \\n{}\".format(true_positives + false_negatives + false_positives + true_negatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (true_positives + true_negatives) / (true_positives + false_negatives + false_positives + true_negatives)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = true_positives / (true_positives + false_positives)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = true_positives / (true_positives + false_negatives)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_beta_score = (1 + arguments[\"f_score_beta\"] ** 2) * (precision * recall) / (arguments[\"f_score_beta\"] ** 2 * precision + recall)\n",
    "f_beta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_time_anomaly_thresholds[np.argmax(f_beta_score)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_validation_mixed_predictions_mahalanobis_distance_batch_features = np.stack(\n",
    "  arrays = [prediction[\"mahalanobis_distance_features\"] \n",
    "            for prediction in validation_mixed_predictions_list], axis = 0)\n",
    "print(\"arr_validation_mixed_predictions_mahalanobis_distance_batch_features.shape = {}\".format(arr_validation_mixed_predictions_mahalanobis_distance_batch_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_normal_mahalanobis_distance_batch_features = np.min(np.max(\n",
    "  arr_validation_mixed_predictions_mahalanobis_distance_batch_features[labels_normal_mask, :], axis = -1))\n",
    "max_normal_mahalanobis_distance_batch_features = np.max(np.max(\n",
    "  arr_validation_mixed_predictions_mahalanobis_distance_batch_features[labels_normal_mask, :], axis = -1))\n",
    "print(\"min_normal_mahalanobis_distance_batch_features = {} & max_normal_mahalanobis_distance_batch_features = {}\".format(min_normal_mahalanobis_distance_batch_features, max_normal_mahalanobis_distance_batch_features))\n",
    "\n",
    "min_anomalous_mahalanobis_distance_batch_features = np.min(np.max(\n",
    "  arr_validation_mixed_predictions_mahalanobis_distance_batch_features[labels_anomalous_mask, :], axis = -1))\n",
    "max_anomalous_mahalanobis_distance_batch_features = np.max(np.max(\n",
    "  arr_validation_mixed_predictions_mahalanobis_distance_batch_features[labels_anomalous_mask, :], axis = -1))\n",
    "print(\"min_anomalous_mahalanobis_distance_batch_features = {} & max_anomalous_mahalanobis_distance_batch_features = {}\".format(min_anomalous_mahalanobis_distance_batch_features, max_anomalous_mahalanobis_distance_batch_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_anomaly_thresholds = arguments[\"num_features_anomaly_thresholds\"];\n",
    "batch_features_anomaly_thresholds = np.linspace(start = arguments[\"min_features_anomaly_threshold\"], \n",
    "                                            stop = arguments[\"max_features_anomaly_threshold\"], \n",
    "                                            num = num_features_anomaly_thresholds)\n",
    "print(\"batch_features_anomaly_thresholds.shape = {}\".format(batch_features_anomaly_thresholds.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_validation_mixed_predictions_mahalanobis_distance_batch_features_anomalies_multi_thresholds = np.stack(\n",
    "  arrays = [arr_validation_mixed_predictions_mahalanobis_distance_batch_features > batch_features_anomaly_threshold \n",
    "            for batch_features_anomaly_threshold in batch_features_anomaly_thresholds], axis = -1)\n",
    "print(\"arr_validation_mixed_predictions_mahalanobis_distance_batch_features_anomalies_multi_thresholds.shape = {}\".format(arr_validation_mixed_predictions_mahalanobis_distance_batch_features_anomalies_multi_thresholds.shape))\n",
    "arr_validation_mixed_predictions_mahalanobis_distance_batch_features_anomalies_multi_thresholds = np.any(\n",
    "  a = arr_validation_mixed_predictions_mahalanobis_distance_batch_features_anomalies_multi_thresholds, axis = 1)\n",
    "print(\"arr_validation_mixed_predictions_mahalanobis_distance_batch_features_anomalies_multi_thresholds.shape = {}\".format(arr_validation_mixed_predictions_mahalanobis_distance_batch_features_anomalies_multi_thresholds.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_normals = arr_validation_mixed_predictions_mahalanobis_distance_batch_features_anomalies_multi_thresholds == 0\n",
    "print(\"predicted_normals.shape = {}\".format(predicted_normals.shape))\n",
    "predicted_anomalies = arr_validation_mixed_predictions_mahalanobis_distance_batch_features_anomalies_multi_thresholds == 1\n",
    "print(\"predicted_anomalies.shape = {}\".format(predicted_anomalies.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = np.sum(\n",
    "  a = np.stack(arrays = [\n",
    "    np.logical_and(\n",
    "      labels_anomalous_mask, predicted_anomalies[:, threshold]) \n",
    "    for threshold in range(num_features_anomaly_thresholds)], \n",
    "               axis = -1), \n",
    "  axis = 0)\n",
    "\n",
    "false_negatives = np.sum(\n",
    "  a = np.stack(arrays = [\n",
    "    np.logical_and(\n",
    "      labels_anomalous_mask, predicted_normals[:, threshold]) \n",
    "    for threshold in range(num_features_anomaly_thresholds)], \n",
    "               axis = -1), \n",
    "  axis = 0)\n",
    "\n",
    "false_positives = np.sum(\n",
    "  a = np.stack(arrays = [\n",
    "    np.logical_and(\n",
    "      labels_normal_mask, predicted_anomalies[:, threshold]) \n",
    "    for threshold in range(num_features_anomaly_thresholds)], \n",
    "               axis = -1), \n",
    "  axis = 0)\n",
    "\n",
    "true_negatives = np.sum(\n",
    "  a = np.stack(arrays = [np.logical_and(\n",
    "    labels_normal_mask, predicted_normals[:, threshold]) \n",
    "                         for threshold in range(num_features_anomaly_thresholds)], \n",
    "               axis = -1), \n",
    "  axis = 0)\n",
    "print(\"true_positives.shape = {}, false_negatives.shape = {}, false_positives.shape = {}, true_negatives.shape = {}\".format(true_positives.shape, false_negatives.shape, false_positives.shape, true_negatives.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"true_positives = \\n{}\".format(true_positives))\n",
    "print(\"false_negatives = \\n{}\".format(false_negatives))\n",
    "print(\"false_positives = \\n{}\".format(false_positives))\n",
    "print(\"true_negatives = \\n{}\".format(true_negatives))\n",
    "print(\"true_positives + false_negatives + false_positives + true_negatives = \\n{}\".format(true_positives + false_negatives + false_positives + true_negatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (true_positives + true_negatives) / (true_positives + false_negatives + false_positives + true_negatives)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = true_positives / (true_positives + false_positives)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = true_positives / (true_positives + false_negatives)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_beta_score = (1 + arguments[\"f_score_beta\"] ** 2) * (precision * recall) / (arguments[\"f_score_beta\"] ** 2 * precision + recall)\n",
    "f_beta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_features_anomaly_thresholds[np.argmax(f_beta_score)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_labeled_test_mixed_sequences = np.genfromtxt(fname = \"data/labeled_test_mixed_sequences.csv\", delimiter = ';', dtype = str)\n",
    "arr_labeled_test_mixed_sequences_features = np.stack(\n",
    "  arrays = [np.stack(\n",
    "    arrays = [np.array(arr_labeled_test_mixed_sequences[example_index, tag_index].split(',')).astype(np.float) \n",
    "              for tag_index in range(number_of_tags)], axis = 1) \n",
    "            for example_index in range(len(arr_labeled_test_mixed_sequences))], axis = 0)\n",
    "dict_labeled_test_mixed_sequences_features = {tag: arr_labeled_test_mixed_sequences_features[:, :, index] for index, tag in enumerate(UNLABELED_CSV_COLUMNS)}\n",
    "arr_test_labels = arr_labeled_test_mixed_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = [prediction for prediction in estimator.predict(\n",
    "  input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x = dict_labeled_test_mixed_sequences_features,\n",
    "    y = None,\n",
    "    batch_size = 128,\n",
    "    num_epochs = 1,\n",
    "    shuffle = False,\n",
    "    queue_capacity = 1000))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_test_labels[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_test_example_index = np.argmax(arr_test_labels == '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list[normal_test_example_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "for i, arr in enumerate(np.split(ary = predictions_list[normal_test_example_index][\"X_time_abs_recon_err\"].flatten(), indices_or_sections = len(UNLABELED_CSV_COLUMNS), axis = 0)):\n",
    "  sns.tsplot(arr, color = flatui[i%len(flatui)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "for i, arr in enumerate(np.split(ary = np.transpose(a = predictions_list[normal_test_example_index][\"X_features_abs_recon_err\"]).flatten(), indices_or_sections = len(UNLABELED_CSV_COLUMNS), axis = 0)):\n",
    "  sns.tsplot(arr, color = flatui[i%len(flatui)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomalous Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_test_example_index = np.argmax(arr_test_labels == '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list[anomalous_test_example_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "for i, arr in enumerate(np.split(ary = predictions_list[anomalous_test_example_index][\"X_time_abs_recon_err\"].flatten(), indices_or_sections = len(UNLABELED_CSV_COLUMNS), axis = 0)):\n",
    "  sns.tsplot(arr, color = flatui[i%len(flatui)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "for i, arr in enumerate(np.split(ary = np.transpose(a = predictions_list[anomalous_test_example_index][\"X_features_abs_recon_err\"]).flatten(), indices_or_sections = len(UNLABELED_CSV_COLUMNS), axis = 0)):\n",
    "  sns.tsplot(arr, color = flatui[i%len(flatui)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
